[{"uri":"https://docvoxx.github.io/internship-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Vietnam Cloud Day Event Objectives Equip executive leadership with strategic insights to navigate the Generative AI era. Disseminate best practices for establishing a unified, scalable data foundation on AWS. Present the AI-Driven Development Lifecycle (AI-DLC) and its transformative role in software engineering. Examine core security principles for Generative AI and the evolution toward autonomous AI Agents. Speakers Eric Yeo – Country GM, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – VP \u0026amp; GM APJ, AWS Panelists: Jeff Johnson, Vu Van (ELSA), Nguyen Hoa Binh (Nexttech), Dieter Botha (TymeX) AWS Specialists: Kien Nguyen, Jun Kai Loke, Tamelly Lim, Binh Tran, Taiki Dang, Michael Armentano Key Highlights 1. Strategic Leadership \u0026amp; Vision Keynote Sessions: Leaders from AWS, Techcombank, and U2U Network outlined their vision for cloud and AI adoption across the region. Executive Panel: A discussion titled \u0026ldquo;Navigating the GenAI Revolution\u0026rdquo; focused on building an innovation-centric culture, aligning AI with business strategy, and managing organizational change during AI integration. 2. Data Foundation \u0026amp; Roadmap Unified Data Foundation: This session detailed the construction of a robust infrastructure for data ingestion, storage, and governance—a mandatory prerequisite for effective AI workloads. GenAI Roadmap: AWS showcased its comprehensive vision and emerging tools designed to empower organizations to drive efficiency through GenAI. 3. The Future of Software Development AI-Driven Development Lifecycle (AI-DLC): Discussions centered on a shift where AI acts not merely as an assistant but as a central collaborator. This model combines AI execution with human oversight to accelerate innovation beyond traditional methodologies. 4. Security \u0026amp; Advanced Automation Securing GenAI: Security was addressed across three layers: infrastructure, models, and applications, emphasizing encryption, zero-trust architecture, and granular access controls. AI Agents: The event concluded with a focus on the shift from basic automation to Intelligent Agents—systems capable of learning, adapting, and executing complex tasks autonomously. Key Takeaways Cultural Shift Adopting AI-DLC: Software development is evolving from human-led efforts with AI assistance to AI-centric collaboration, requiring teams to adapt their coding and testing approaches. Agents vs. Automation: There is a fundamental difference between static automation scripts and dynamic AI Agents that can make decisions based on changing inputs. Technical Pillars Data First: A unified and governed data foundation is critical for GenAI success. Security by Design: Security measures must be continuous and layered to ensure data confidentiality throughout the AI lifecycle. Applying to Work Assess Data Readiness: Evaluate the current AWS data infrastructure to ensure it meets the scalability and governance standards required for GenAI (referencing the Unified Data Foundation session). Explore AI Agents: Identify complex manual operations that are suitable for offloading to autonomous AI Agents rather than simple scripts. Adopt AI-DLC: Experiment with integrating AI tools more deeply into the development lifecycle, treating them as collaborators rather than just code completion utilities. Event Experience The summit offered a holistic perspective on the GenAI landscape, effectively balancing high-level strategy with technical depth.\nStrategic Insight: The panel featuring leaders from ELSA, Nexttech, and TymeX provided valuable real-world context on managing the cultural impact of AI. Technical Depth: The afternoon tracks were highly relevant, particularly the deep dives into AI-DLC and Securing GenAI, which directly align with our technical roadmap. Some event photos Add your event photos here\n"},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Create Lambda Functions","tags":[],"description":"","content":"Step 1: Create IAM Role for Lambda Go to IAM Console → Roles → Create role\nTrusted entity type:\nAWS service Use case: Lambda Add permissions:\nAWSLambdaVPCAccessExecutionRole AWSLambdaBasicExecutionRole Role details:\nRole name: daivietblood-lambda-role Description: IAM role for DaiVietBlood Lambda functions Click Create role\nStep 2: Create Lambda Layer for Dependencies Create a folder for dependencies: mkdir -p nodejs cd nodejs npm init -y npm install mysql2 Create zip file: cd .. zip -r mysql2-layer.zip nodejs Go to Lambda Console → Layers → Create layer\nConfigure:\nName: mysql2-layer Upload: Select mysql2-layer.zip Compatible runtimes: Node.js 18.x, Node.js 20.x Click Create\nStep 3: Create Lambda Function - Get Users Go to Lambda Console → Functions → Create function\nBasic information:\nFunction name: daivietblood-get-users Runtime: Node.js 20.x Architecture: x86_64 Execution role: Use existing role → daivietblood-lambda-role Click Create function\nAdd Layer:\nScroll to Layers → Add a layer Custom layers → Select mysql2-layer Click Add Configure VPC:\nGo to Configuration → VPC → Edit VPC: daivietblood-vpc Subnets: Select both Private Subnets Security groups: daivietblood-lambda-sg Click Save Add Environment Variables:\nGo to Configuration → Environment variables → Edit Add: DB_HOST = daivietblood-db.xxxx.ap-southeast-1.rds.amazonaws.com DB_PORT = 3306 DB_NAME = daivietblood DB_USER = admin DB_PASSWORD = YourSecurePassword123! Click Save Add code in Code tab:\nconst mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const conn = await getConnection(); const [rows] = await conn.execute(\u0026#39;SELECT * FROM users\u0026#39;); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Click Deploy Step 4: Create Lambda Function - Create User Create new function: daivietblood-create-user Same configuration as above (VPC, Layer, Environment Variables) Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const body = JSON.parse(event.body); const { email, name, blood_type, phone } = body; if (!email || !name || !blood_type) { return { statusCode: 400, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Missing required fields\u0026#39; }) }; } const conn = await getConnection(); const [result] = await conn.execute( \u0026#39;INSERT INTO users (email, name, blood_type, phone) VALUES (?, ?, ?, ?)\u0026#39;, [email, name, blood_type, phone || null] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, email, name, blood_type, phone }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); if (error.code === \u0026#39;ER_DUP_ENTRY\u0026#39;) { return { statusCode: 409, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Email already exists\u0026#39; }) }; } return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Step 5: Create Lambda Function - Emergency Requests Create function: daivietblood-emergency-requests Same configuration Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { const conn = await getConnection(); const method = event.httpMethod; try { if (method === \u0026#39;GET\u0026#39;) { const [rows] = await conn.execute( \u0026#39;SELECT * FROM emergency_requests WHERE status = \u0026#34;open\u0026#34; ORDER BY urgency DESC, created_at DESC\u0026#39; ); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } if (method === \u0026#39;POST\u0026#39;) { const body = JSON.parse(event.body); const { requester_name, blood_type, units_needed, hospital, urgency } = body; const [result] = await conn.execute( \u0026#39;INSERT INTO emergency_requests (requester_name, blood_type, units_needed, hospital, urgency) VALUES (?, ?, ?, ?, ?)\u0026#39;, [requester_name, blood_type, units_needed, hospital, urgency || \u0026#39;normal\u0026#39;] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, message: \u0026#39;Emergency request created\u0026#39; }) }; } return { statusCode: 405, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Method not allowed\u0026#39; }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Verification Checklist IAM Role created with VPC and Basic execution permissions Lambda Layer created with mysql2 package Lambda functions created and deployed: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests All functions configured with VPC (Private Subnets) Environment variables set correctly Functions deployed successfully "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create VPC","tags":[],"description":"","content":"Step 1: Create VPC Go to VPC Console → Your VPCs → Create VPC\nConfigure VPC:\nResources to create: VPC and more Name tag auto-generation: daivietblood IPv4 CIDR block: 10.0.0.0/16 IPv6 CIDR block: No IPv6 CIDR block Tenancy: Default Configure Subnets:\nNumber of Availability Zones: 2 Number of public subnets: 2 Number of private subnets: 2 Customize subnets CIDR blocks: Public subnet CIDR block in ap-southeast-1a: 10.0.1.0/24 Public subnet CIDR block in ap-southeast-1b: 10.0.2.0/24 Private subnet CIDR block in ap-southeast-1a: 10.0.3.0/24 Private subnet CIDR block in ap-southeast-1b: 10.0.4.0/24 Configure NAT Gateway:\nNAT gateways: In 1 AZ Configure VPC Endpoints:\nVPC endpoints: None (we\u0026rsquo;ll create later if needed) Click Create VPC\nℹ️ VPC creation takes 2-3 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 2: Verify VPC Resources After creation, verify the following resources were created:\nResource Name Details VPC daivietblood-vpc 10.0.0.0/16 Public Subnet 1 daivietblood-subnet-public1-ap-southeast-1a 10.0.1.0/24 Public Subnet 2 daivietblood-subnet-public2-ap-southeast-1b 10.0.2.0/24 Private Subnet 1 daivietblood-subnet-private1-ap-southeast-1a 10.0.3.0/24 Private Subnet 2 daivietblood-subnet-private2-ap-southeast-1b 10.0.4.0/24 Internet Gateway daivietblood-igw Attached to VPC NAT Gateway daivietblood-nat-public1-ap-southeast-1a In Public Subnet 1 Route Table (Public) daivietblood-rtb-public Routes to IGW Route Table (Private) daivietblood-rtb-private1-ap-southeast-1a Routes to NAT Step 3: Create Security Groups 3.1. Security Group for Lambda\nGo to VPC Console → Security Groups → Create security group\nConfigure:\nSecurity group name: daivietblood-lambda-sg Description: Security group for Lambda functions VPC: Select daivietblood-vpc Inbound rules: (Leave empty - Lambda initiates connections)\nOutbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n3.2. Security Group for RDS\nGo to VPC Console → Security Groups → Create security group\nConfigure:\nSecurity group name: daivietblood-rds-sg Description: Security group for RDS MySQL VPC: Select daivietblood-vpc Inbound rules:\nType Protocol Port Source Description MySQL/Aurora TCP 3306 daivietblood-lambda-sg Allow Lambda access Outbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n⚠️ Security Best Practice: Only allow access from Lambda Security Group to RDS. Never open port 3306 to 0.0.0.0/0.\nStep 4: Create DB Subnet Group Go to RDS Console → Subnet groups → Create DB subnet group\nConfigure:\nName: daivietblood-db-subnet-group Description: Subnet group for DaiVietBlood RDS VPC: Select daivietblood-vpc Add subnets:\nAvailability Zones: Select ap-southeast-1a and ap-southeast-1b Subnets: Select both Private Subnets (10.0.3.0/24 and 10.0.4.0/24) Click Create\nVerification Checklist VPC created with CIDR 10.0.0.0/16 2 Public Subnets created 2 Private Subnets created Internet Gateway attached to VPC NAT Gateway created in Public Subnet Route tables configured correctly Lambda Security Group created RDS Security Group created with inbound rule from Lambda SG DB Subnet Group created with Private Subnets "},{"uri":"https://docvoxx.github.io/internship-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nData Resiliency in a Cloud-first World Event Objectives Share best practices in modern application design. Introduce DDD methods and event-driven architecture. Guide on selecting appropriate compute services. Introduce AI tools supporting the development lifecycle. List of Speakers Paul Haverfield - Principal Storage Specialist BDM, APJ Tamelly Lim - Specialist Solutions Architect Ameen Khan S - GTM specialist for Storage - Data \u0026amp; AI pillar covering ASEAN markets Paul Hidalgo - Key Highlights Today, I had the opportunity to attend an AWS program covering an extremely urgent topic in the current context: Data Resiliency. More than just simple backups, the event opened up new perspectives on protecting digital assets against increasingly sophisticated threats.\nHere are the key takeaways I gathered:\n1. Redefining: How is Data Resiliency different from High Availability (HA) and Disaster Recovery (DR)? Previously, we often focused on HA (ensuring systems are always online) or DR (recovering from physical disasters). However, Data Resiliency is a broader and more \u0026ldquo;proactive\u0026rdquo; concept:\nContext: \u0026ldquo;Everything fails, all the time\u0026rdquo; (Werner Vogels) – Everything is prone to failure, including physical keys or hardware. The Difference: While HA handles infrastructure incidents, Data Resiliency focuses on data integrity. It is the organizational ability to maintain operations, withstand, and recover even when under cyberattack (such as Ransomware) or human error. Goal: To detect anomalies and automate response processes without human intervention. 2. Why has Data Resiliency become an \u0026ldquo;Absolute Necessity\u0026rdquo;? The explosion of data creation comes with new technological vulnerabilities. Three main trends are driving the shift from Protection to Resiliency:\nRegulatory: Compliance with strict data protection laws. Technology: The complexity of Multi-cloud and Hybrid-cloud environments. Threat Landscape: Ransomware no longer just encrypts primary data; it also targets backups. 3. Data Immutability: The Impenetrable Shield A keyword mentioned repeatedly was Data Immutability.\nThis refers to the ability to create data copies that cannot be changed or deleted for a set period. In the event of a Ransomware attack, even if hackers possess top-level admin rights, they cannot alter this backup. It acts as the \u0026ldquo;Last line of defense,\u0026rdquo; ensuring that at least one clean version always exists for recovery. 4. Protection Strategy: The AWS 3-2-1-1-0 Model The traditional 3-2-1 backup rule has been upgraded to suit the cloud era:\n3 copies of data. 2 different storage media. 1 off-site copy (different region). 1 offline or Immutable (Air-gapped) copy. 0 errors during recovery (verified by automated testing). Important concepts to remember:\nRPO (Recovery Point Objective): How much data loss is acceptable? RTO (Recovery Time Objective): How long does it take to get the system running again? Backup Vault: A container for storing backups, encrypted by AWS KMS for enhanced security. 5. Tool Ecosystem \u0026amp; Solutions The event introduced powerful integrated solutions on AWS:\nCommvault Cloud on AWS: Provides Air Gap Protect (secure data isolation). Cloud Rewind: The ability to \u0026ldquo;rewind\u0026rdquo; time to restore entire instances or VPCs as if the incident never happened. Clumio: An all-in-one simplified backup solution. Uses Serverless Workflow architecture (an army of Lambda functions) to optimize costs and operations. Elastio: Focuses on: Detect, Respond, Recover. Scans for Malware/Ransomware directly within Snapshots to ensure backups do not contain latent malicious code. 6. Workshop Architecture: Real-world Implementation During the hands-on session, we deployed a comprehensive protection model:\nKey Components:\nSource: EC2 Instances (EBS) and S3 Buckets containing critical data. Mechanism: Using AWS Backup Plan with an hourly schedule. Protection Layers: Primary: Stored in a standard Vault (workshop-sources-regular-vault). Secondary (Air-gapped): Copied to another Region (us-east-1-LAG-Vault) with Immutability settings enabled. Validation: Integrated Elastio with AWS Backup. Automated Malware scanning on backups. Performed hourly Restore Testing to ensure backup viability (the \u0026ldquo;0 error\u0026rdquo; strategy). Conclusion The event shifted my mindset from merely \u0026ldquo;backing up data\u0026rdquo; to \u0026ldquo;building resiliency.\u0026rdquo; In an era where cyberattacks are inevitable, possessing a Data Resiliency strategy featuring Immutability and Automation (using tools like Elastio or Commvault) is vital for business survival.\nSome photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://docvoxx.github.io/internship-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nData Science on AWS Event Objectives Share essential services for data processing (sentiment analysis, comment classification, etc.). List of Speakers Van Hoang Kha - Cloud Solutions Architect, AWS Community Builder Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights Below is a summary of the event content focusing on the listed services, presented in a professional report style without icons as requested.\nAWS TRAINING PROGRAM SUMMARY REPORT: AI \u0026amp; MACHINE LEARNING\n1. Overview of Technology Concepts\nTo begin the program, we systematized important foundational concepts in the field of intelligent technology:\nAI (Artificial Intelligence): An overarching concept regarding the creation of intelligent systems. ML (Machine Learning): A subset of AI that allows computers to learn from data. DL (Deep Learning): Uses complex neural networks to model patterns in data. GenAI (Generative AI): Focuses on creating new content and data. 2. AWS as a Service Provider\nThe next section introduced AWS as a comprehensive service provider. AWS offers Managed Services that help businesses apply AI quickly without investing heavily in building infrastructure from scratch.\n3. Details of Introduced AWS Services\nThe training focused deeply on specific tools designed to solve real-world business problems:\nAmazon Comprehend (Natural Language Processing Service - NLP) This service was discussed in the most detail, featuring powerful multi-language text processing capabilities:\nSentiment Analysis: Automatically classifies customer reviews and comments based on positive, negative, or neutral nuances. Text Summarization: Condenses content from long documents. Large-scale Information Processing: Supports bulk email processing and classification. Information Security: Capable of identifying, classifying, and protecting sensitive Personally Identifiable Information (PII) within text. Other Language and Text Processing Services\nAmazon Translate: Automated language translation service. Amazon Textract: A tool for extracting data from scanned documents and papers, including handwriting and complex forms. Amazon Transcribe: A service for converting speech (audio) into written text. Image and Computer Vision Services\nAmazon Rekognition: A Deep Learning-based service specialized for analyzing images and videos (object detection, facial recognition, content moderation). Customer Experience Services\nAmazon Personalize: A solution to enhance customer experience through personalization. This service records and analyzes user behavior, thereby providing product or content recommendations best suited to individual preferences. Technical Infrastructure\nSageMaker Instance: Provides the server environment and tools necessary for developers to self-build, train, and deploy custom machine learning models according to specific needs. Some photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://docvoxx.github.io/internship-report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only; please do not copy it verbatim for your report, including this warning.\nSummary Report: Attending AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nAttendance Objectives Recently, I had the opportunity to attend the opening event for the \u0026ldquo;AWS Cloud Mastery\u0026rdquo; series, focusing on AI, Machine Learning, and Generative AI. My main goal was to update my comprehensive view of these technologies on the AWS platform and learn how to apply them to real-world business problems.\nSpeakers The session featured sharing from experienced experts in the industry, including Mr. Lam Tuan Kiet (Sr DevOps Engineer - FPT Software), Mr. Danh Hoang Hieu Nghi (AI Engineer - Renova Cloud), Mr. Dinh Le Hoang Anh (Cloud Engineer Trainee), and Mr. Van Hoang Kha (Community Builder).\nValuable Knowledge I Harvested:\n1. The Power of Generative AI on Amazon Bedrock This was the part that impressed me the most. Amazon Bedrock acts as a central platform, providing access to leading Foundation Models (FMs) from Anthropic, OpenAI, Meta, etc. This allows us to fine-tune existing models without having to build a model from scratch.\nI also reinforced my skills in Prompt Engineering, understanding better how to guide the model through various strategies:\nZero-shot: Providing a request directly without examples. Few-shot: Providing a handful of examples for the model to mimic. Chain-of-Thought: Asking the model to explain its reasoning steps for a more logical result. Specifically, the RAG (Retrieval Augmented Generation) technique was highlighted as an optimal solution to improve accuracy:\nRetrieval: Pulling real data from the enterprise knowledge base. Augmentation: Adding that data as context for the prompt. Generation: The model answers based on factual information, minimizing hallucinations. Additionally, Amazon Titan Embeddings was introduced as a tool to convert text into vectors, serving semantic search and multilingual RAG workflows.\n2. AWS AI Services Ecosystem Beyond GenAI, I also reviewed AWS\u0026rsquo;s \u0026ldquo;ready-made\u0026rdquo; AI services (APIs) that help solve specific problems quickly without complex model training:\nImage/Video Analysis (Rekognition). Translation (Translate) and Speech-to-Text/Text-to-Speech (Transcribe, Polly). Data Extraction (Textract) and Natural Language Processing (Comprehend). Intelligent Search (Kendra) or Anomaly Detection (Lookout). The AMZPhoto face recognition demo visually illustrated how to integrate these services into a real product.\n3. Amazon Bedrock AgentCore – Putting AI Agents into Practice This is a new framework helping to solve the problem of operating AI Agents at scale (production-ready). It supports long-term memory management, identity security, tool integration (like browsers, code interpreters), and most importantly, observability. This makes deploying frameworks like CrewAI or LangGraph safer and more effective on the AWS platform.\nPlan for Application at Work Based on what I learned, I plan to apply the following knowledge immediately:\nDeploy RAG \u0026amp; AgentCore: Propose and apply these to upcoming internal projects requiring GenAI features to increase accuracy and automation capabilities. Optimize Development Process: Use available AWS AI Services instead of building from scratch to shorten Time-to-market. Improve Model Performance: Apply advanced Prompt Engineering techniques to optimize output results for current AI tasks. Side Experience Not only did I absorb knowledge, but the event atmosphere was also extremely lively. I was lucky enough to reach the Top 6 in the Kahoot competition; however, I dropped off the leaderboard during the final questions. Nevertheless, it was still an interesting game that helped me reinforce the knowledge from this event.\n"},{"uri":"https://docvoxx.github.io/internship-report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only; please do not copy it verbatim for your report, including this warning.\nSummary Report: Attending AWS Cloud Mastery Series #2 – DevOps on AWS Attendance Objectives Continuing the event series, I attended the second session focusing on DevOps. My goal was to master AWS services supporting DevOps, deepen my understanding of CI/CD pipeline design, Infrastructure as Code (IaC) concepts, as well as how to deploy and monitor containerized applications on the AWS platform.\nSpeakers The session gathered a strong lineup of speakers, including AWS Community Builders and experienced engineers:\nMr. Truong Quang Tinh – Platform Engineer (TymeX) Mr. Bao Huynh, Nguyen Khanh Phuc Thinh, Tran Dai Vi, Huynh Hoang Long, Pham Hoang Quy, Nghiem Le (AWS Community Builders) Mr. Dinh Le Hoang Anh – Cloud Engineer Trainee (First Cloud AI Journey) Valuable Knowledge I Harvested: 1. Building a DevOps Foundation The speakers helped me redefine that DevOps is not a job title but a mindset and habit of working. The core lies in:\nAutomating repetitive tasks. Sharing knowledge across teams. Continuously experimenting and learning. Measuring effectiveness with real data rather than assumptions. I also learned lessons about common pitfalls for beginners: avoid getting stuck in \u0026ldquo;tutorial hell\u0026rdquo; without starting real projects, and focus on personal progress rather than comparing oneself to others.\n2. Infrastructure as Code (IaC) in Practice This section broadened my horizon regarding IaC tools instead of sticking to a single one. The speakers provided a detailed comparison:\nCloudFormation: AWS\u0026rsquo;s native template tool. AWS CDK: For developers who prefer writing infrastructure using familiar programming languages. Terraform: The optimal choice for teams working on multi-cloud platforms. The most important message: Infrastructure built via Code (IaC) is significantly more consistent, maintainable, and secure compared to manual configuration (ClickOps).\n3. Containers and Deployment Models The content ranged from basics (Dockerfile, Image) to advanced AWS services:\nAmazon ECR: Storage and security scanning for images. Amazon ECS \u0026amp; EKS: Two popular container orchestration options. The comparison between ECS (simple, native) and EKS (powerful with Kubernetes) helped me know when to use which. AWS App Runner: A quick deployment solution without worrying about cluster management. 4. Monitoring and Observability A system cannot lack monitoring capabilities. I understood better the roles of:\nAmazon CloudWatch: The center for metrics, logs, and alarms. AWS X-Ray: A tracing tool helping visualize request flows and detect bottlenecks. Core lesson: Observability features must be designed from the very beginning, not added after the system is built.\nPlan for Application at Work Specifically, I plan to apply this knowledge to the team\u0026rsquo;s upcoming AI Chatbot Project:\nEstablish CI/CD Pipeline: Use AWS CodePipeline to automate the entire process from Build, Test, to Deploy. The goal is to ensure every code update is tested and pushed to production smoothly. Implement IaC: Use AWS CDK to define all resources (Lambda, API Gateway, DynamoDB, S3, IAM\u0026hellip;). This makes the system easy to reuse for different environments and scale quickly when needed. Applying this process will help the Chatbot project develop faster, minimize human error, and make operations easier.\nEvent Experience The session gave me a practical perspective on how modern businesses implement DevOps on AWS. Beyond theory, the real-world examples from speakers about CloudFormation, Terraform, or how to choose EKS/ECS were truly valuable.\nBeyond professional knowledge, this was also a good occasion for me to network with like-minded friends and learn \u0026ldquo;hard-earned\u0026rdquo; experiences from those who came before me.\n"},{"uri":"https://docvoxx.github.io/internship-report/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nEvent Summary Report: AWS Cloud Mastery Series #3 Topic: AWS Well-Architected – Security Pillar Workshop\n1. Overview \u0026amp; Speakers The event focused on the most critical pillar within the AWS Well-Architected Framework: Security. The content was designed to equip attendees with knowledge ranging from fundamental to advanced levels regarding identity, monitoring, infrastructure protection, data protection, and incident response processes.\nSpeakers \u0026amp; Experts: The event gathered experts from the AWS Community (AWS Community Builders), AWS Cloud Club Captains from various universities (HCMUTE, SGU, PTIT, HUFLIT), Cloud Engineers from FCJ, and specially featuring Mendel Grabski (Security \u0026amp; DevOps Expert).\nAbout AWS Cloud Club: This is a network connecting students and professionals, helping to develop technical leadership skills, providing hands-on experiences, and offering long-term mentoring opportunities. The participating Cloud Clubs under FCJA include: HCMUTE, SGU, PTIT, and HUFLIT.\n2. Key Technical Highlights A. Identity \u0026amp; Access Management (IAM) IAM is defined as the \u0026ldquo;first line of defense.\u0026rdquo; The session emphasized the shift from manual management to automation and strict adherence to key principles:\nLeast Privilege Principle: Grant only the necessary permissions required to perform a task. Root User Protection: Delete access keys immediately after creation. Service Control Policies (SCPs): Use Organization-level policies to set a \u0026ldquo;ceiling\u0026rdquo; (maximum available permissions) for member accounts (Note: SCPs only filter permissions; they do not grant them). Permission Boundaries: Set the maximum permissions that an identity-based policy can grant to a specific User/Role. Multi-Factor Authentication (MFA): Encouraged the use of FIDO2 (hardware keys/biometrics) over traditional TOTP. Credential Rotation: Use AWS Secrets Manager to automate the rotation process (create -\u0026gt; set -\u0026gt; test -\u0026gt; finish) and integrate with EventBridge to manage schedules, eliminating risks associated with \u0026ldquo;hardcoded credentials.\u0026rdquo; B. Continuous Monitoring \u0026amp; Threat Detection The focus was on building comprehensive visibility and automated response capabilities:\nMulti-Layer Monitoring: Combining CloudTrail (recording API calls, S3/Lambda access) and VPC Flow Logs (network traffic). Event-Driven Security: Using EventBridge as a central event bus to route real-time alerts to Lambda/SNS/SQS or coordinate actions across different accounts (Cross-account routing). Detection-as-Code: Managing detection rules and queries (CloudTrail Lake queries) as code (version control) to ensure consistent deployment across the organization. Deep Dive into Amazon GuardDuty: This is an intelligent threat detection solution that operates continuously based on three main data sources: CloudTrail, VPC Flow Logs, and DNS Logs.\nExpanded Coverage: Protection for S3, EKS, RDS (brute-force detection), Lambda (suspicious network activity), and Malware Protection (EBS scanning). Runtime Monitoring: Uses an Agent to monitor deep inside the OS (processes, file access) on EC2/EKS/Fargate. C. Compliance \u0026amp; Infrastructure as Code (IaC) Security compliance is no longer a manual check but is integrated into the deployment pipeline:\nApplied Standards: AWS Foundational Security Best Practices, CIS Benchmark, PCI DSS, NIST. Enforcement Mechanism: Using AWS CloudFormation (IaC) to deploy standard configurations, combined with AWS Security Hub to automatically audit resources against defined standards. D. Network \u0026amp; Data Protection Network Security: Clearly distinguishing between Security Groups (Stateful - instance level firewall) and NACLs (Stateless - subnet level firewall). Introduction to AWS Network Firewall for advanced Egress filtering/IPS and integration with Threat Intelligence to automatically block malicious traffic. Data Protection: Encryption: Using KMS with Customer Master Keys (CMK) and Policy conditions to control decryption contexts. Certificates: Using AWS ACM to manage and automatically renew SSL/TLS certificates. Service Security: Enforcing HTTPS/TLS 1.2+ for S3 (via Bucket Policy) and Databases (e.g., PostgreSQL rds.force_ssl=1). E. Incident Response (IR) The standard IR process consists of 5 steps: Preparation -\u0026gt; Detection \u0026amp; Analysis -\u0026gt; Containment -\u0026gt; Eradication \u0026amp; Recovery -\u0026gt; Post-Incident Activity.\nPrevention Strategy: Never make S3 buckets public, isolate sensitive services in private subnets, and ensure all infrastructure changes go through IaC with a review process (double-gate). 3. Practical Experience \u0026amp; Q\u0026amp;A The event provided high practical value, specifically aligning with the \u0026ldquo;Automated Incident Response and Forensics\u0026rdquo; project our team is developing.\nDiscussion Point: During our project testing, the team noticed that Amazon GuardDuty has a latency of about 5 minutes to generate a finding after an incident occurs. We asked about solutions to reduce this latency.\nExpert Answer:\nThe Nature of the Service: The GuardDuty latency is an accepted technical characteristic because the system needs time to analyze large datasets to accurately determine threats and avoid false positives. Alternative Solutions: To achieve near real-time detection, the expert suggested integrating 3rd party solutions like OpenClarity (open source) or building custom anomaly detection logic based on CloudTrail events. Networking: After the event, Mr. Mendel Grabski (Ex-Head of Security \u0026amp; DevOps) expressed interest and offered professional support for our project, opening up valuable collaboration and mentoring opportunities.\nSome photos from the event participation [Add your photos here] Overall, the event not only provided technical knowledge but also changed my mindset regarding application design, system modernization, and effective team collaboration.\n"},{"uri":"https://docvoxx.github.io/internship-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Duc Lan\nPhone Number: 0903842341\nEmail: lannguyen68609@gmail.com\nUniversity: FPT University HCMC\nMajor: Artificial Intelligence\nClass: K19\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 28/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"System Architecture The DaiVietBlood system uses a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nArchitecture Components 1. Network Infrastructure (VPC)\nComponent Description VPC Virtual Private Cloud with CIDR 10.0.0.0/16 Public Subnet Contains NAT Gateway, allows Internet access Private Subnet Contains Lambda, RDS - isolated from Internet NAT Gateway Allows Private Subnet resources to access Internet Internet Gateway Allows Public Subnet to communicate with Internet 2. Application \u0026amp; Data Layer\nService Role AWS Lambda Process business logic (CRUD operations, emergency requests) API Gateway Receive HTTP requests, route to Lambda Amazon RDS MySQL database storing user data, blood inventory Amazon S3 Store static files (images, documents) 3. Frontend \u0026amp; Distribution\nService Role AWS Amplify Host React application CloudFront CDN distributes content globally with low latency 4. DevOps \u0026amp; Monitoring\nService Role CodePipeline Automate CI/CD process CodeBuild Build and test source code CloudWatch Collect logs, metrics, set up alarms Data Flow User Request → CloudFront → Amplify (Frontend) ↓ API Gateway ↓ AWS Lambda (Private Subnet) ↓ Amazon RDS (Private Subnet) Security Model Network Isolation: RDS and Lambda in Private Subnet, no direct Internet access IAM Roles: Each service has minimum required permissions (Least Privilege) Data Encryption: At-rest (RDS, S3) and In-transit (HTTPS) Security Groups: Control inbound/outbound traffic for each resource Workshop Objectives After completing this workshop, you will be able to:\n✅ Create VPC with proper network segmentation ✅ Deploy RDS MySQL in Private Subnet ✅ Build Lambda functions and expose via API Gateway ✅ Configure S3 and CloudFront for static content ✅ Deploy React app with Amplify ✅ Set up CI/CD pipeline ✅ Monitor with CloudWatch "},{"uri":"https://docvoxx.github.io/internship-report/3-blogstranslated/3.2-blog2/","title":"AWS Weekly Roundup: Aurora 10th Anniversary, EC2 R8 Instances, Amazon Bedrock, and More (Aug 25, 2025)","tags":[],"description":"","content":"AWS Blog Weekly Roundup AWS Weekly Roundup: Celebrating 10 Years of Amazon Aurora, EC2 R8 Instances, Amazon Bedrock, and More (25 Aug 2025) By Betty Zheng | 25 Aug 2025\nThis week’s roundup reflects on a decade of database innovation as Amazon Aurora marks its 10th anniversary. Aurora fundamentally changed database architecture by separating compute and storage, and over the years it has evolved from MySQL-compatible roots into a full-featured platform with serverless options, DSQL, I/O‑optimized pricing, zero‑ETL integrations, and support for generative AI workloads.\nNotable releases this week\nAWS Billing and Cost Management: Customizable dashboards to aggregate cost data with widgets from Cost Explorer, Savings Plans, and Reserved Instance reports. Amazon Bedrock: Automatic availability for certain open‑weight OpenAI models and expanded batch inference support (Claude Sonnet 4, GPT‑OSS) to reduce cost for large inference workloads. Amazon EC2 R8i and R8i‑flex: New memory‑optimized instances offering up to ~20% better performance and 2.5× memory bandwidth vs R7i; R8i‑flex provides cost savings for workloads that don’t fully consume compute. Amazon S3: Batch data verification in S3 Batch Operations for large-scale checksum verification without downloading objects. Other items you may find interesting\nAmazon announced DeepFleet, a foundation model for multi‑robot coordination. New guidance on building multi‑agent Strands Agents with minimal code. AWS Security Incident Response added ITSM integrations for Jira and ServiceNow. A digital twin + agentic AI approach to root‑cause analysis in collaboration with NTT DOCOMO. Upcoming events\nAWS Summits (various cities) AWS re:Invent 2025 (Las Vegas, Dec 1–5) AWS Community Days worldwide Author\nBetty Zheng — Senior Developer Advocate at AWS focusing on Cloud Native, Cloud Security, and Generative AI. She has extensive experience building developer-focused content and community engagement.\n"},{"uri":"https://docvoxx.github.io/internship-report/3-blogstranslated/3.1-blog1/","title":"Implementing Fine-Grained Amazon Route 53 Access Using AWS IAM Condition Keys (Part 1)","tags":[],"description":"","content":"Networking \u0026amp; Content Delivery Implementing Fine-Grained Amazon Route 53 Access Using AWS IAM Condition Keys (Part 1) By Daniel Yu | 25 Aug 2025 | Networking \u0026amp; Content Delivery, Security, Identity, \u0026amp; Compliance\nMany organizations adopt a multi-account strategy to support multiple deployment teams. This article is for AWS administrators and network engineers managing DNS access across teams that share an Amazon Route 53 hosted zone (private or public). In shared hosted-zone scenarios, teams often need controlled permissions to allow or deny updates to specific DNS records.\nThis post describes a scalable pattern to grant fine-grained access to Route 53 hosted zones using IAM condition keys and principal tags. The example shows how to allow conditional updates to a subset of DNS records in the same AWS account by comparing requested record names against values stored in IAM principal tags.\nSolution overview\nWe assume familiarity with Route 53 hosted zones, IAM policies, and IAM user tags. The design uses the policy Condition element with Route 53 condition keys (for example route53:ChangeResourceRecordSetsNormalizedRecordNames) and ${aws:PrincipalTag/\u0026lt;key\u0026gt;} to compare requested record names with the tag value attached to the IAM principal. That comparison enables least-privilege updates to record names or suffixes.\nArchitecture\nIAM users receive a custom principal tag (for example dnsrecords) whose value is either a single FQDN (e.g., svc1.example.com) or a suffix. A policy evaluates route53:ChangeResourceRecordSets and uses route53:ChangeResourceRecordSetsNormalizedRecordNames to compare the requested record names with the user tag.\nAllow a specific DNS record (ForAllValues:StringEquals)\nUse ForAllValues:StringEquals to permit modifications only when the requested record name exactly equals the tag value. Example (replace {custom-attribute} and {Route53HostedZoneID}):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:route53:::hostedzone/{Route53HostedZoneID}\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ForAllValues:StringEquals\u0026#34;: { \u0026#34;route53:ChangeResourceRecordSetsNormalizedRecordNames\u0026#34;: [ \u0026#34;${aws:PrincipalTag/{custom-attribute}}\u0026#34; ] } } } ] } If the tag value is svc1.example.com, the user can only manage that record.\nDeny a specific DNS record (ForAllValues:StringNotEquals)\nUse ForAllValues:StringNotEquals to deny updates for a specific record while allowing other changes:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:route53:::hostedzone/{Route53HostedZoneID}\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ForAllValues:StringNotEquals\u0026#34;: { \u0026#34;route53:ChangeResourceRecordSetsNormalizedRecordNames\u0026#34;: [ \u0026#34;${aws:PrincipalTag/{custom-attribute}}\u0026#34; ] } } } ] } Allow a domain suffix (ForAllValues:StringLike)\nUse ForAllValues:StringLike with a wildcard to permit updates within a suffix (e.g., *.svc1.example.com):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:route53:::hostedzone/{Route53HostedZoneID}\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ForAllValues:StringLike\u0026#34;: { \u0026#34;route53:ChangeResourceRecordSetsNormalizedRecordNames\u0026#34;: [ \u0026#34;*.${aws:PrincipalTag/{custom-attribute}}\u0026#34; ] } } } ] } Prerequisites and notes\nPrincipal tag values must not contain commas; a single tag supports a single DNS value. To enable multiple allowed values, use multiple tags and policies. Replace {Route53HostedZoneID} and {custom-attribute} with your actual hosted zone ID and tag key (for example dnsrecords). Workflow: tag IAM users, create the managed policy with the condition keys, attach the policy to users/groups, and verify changes using the Route 53 console or CLI. Example tag command (replace \u0026lt;USER_NAME\u0026gt;):\naws iam tag-user --user-name \u0026lt;USER_NAME\u0026gt; --tags Key=dnsrecords,Value=svc1.example.com This pattern simplifies access control for shared hosted zones by using principal tags and condition keys to implement least-privilege DNS management. Part 2 will cover cross-account and federated-user scenarios.\n"},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.7-blooddonation-workshop/5.7.1-prerequisite/","title":"Prerequisite","tags":[],"description":"","content":"Prerequisites Before starting, ensure you have the following prepared:\nAn AWS account with permissions to create IAM roles, Lambda, API Gateway, RDS, S3, and Code* services. AWS CLI installed and configured (aws configure) with an IAM user that has sufficient privileges. Node.js (v16/18 recommended) and npm installed locally for building the backend. Git installed to clone the repository. A local workstation with zip available to package deployment artifacts. Repository to clone:\ngit clone https://github.com/june4m/aws-blood_donation_cloud.git cd aws-blood_donation_cloud Key files in the repo used by this workshop:\nBloodDonationSupportSystemBE/ - Backend Node.js project (packaging for Lambda) BloodDonationSupportSystem.sql, initData.sql - Database schema \u0026amp; seed data buildspec.yml - Reference CodeBuild pipeline used in CI/CD BloodSupportSystem.md - Project description and architecture notes Security note: do not commit AWS credentials or DB passwords. Use Secrets Manager or environment variables for production.\n"},{"uri":"https://docvoxx.github.io/internship-report/3-blogstranslated/3.3-blog3/","title":"Simpler Hybrid DNS Management with Delegation for Amazon Route 53 Resolver Endpoints","tags":[],"description":"","content":"Networking \u0026amp; Content Delivery Simpler Hybrid DNS Management with Delegation for Amazon Route 53 Resolver Endpoints By Tekena Orugbani | 25 Aug 2025\nAWS announced that Route 53 Resolver endpoints now support DNS delegation. This feature allows you to delegate subdomain management between on‑premises DNS infrastructure and Route 53, simplifying hybrid DNS architectures that previously required customers to operate their own authoritative DNS servers.\nWhy delegation matters\nLarge organizations often run decentralized DNS: a central team manages the parent domain while individual teams manage subdomains. With delegation support, you can delegate a subdomain to Route 53 (or delegate it back to on‑premises), enabling unified private DNS namespaces across on‑premises and AWS without running additional authoritative DNS infrastructure.\nInbound delegation (on‑premises → AWS)\nInbound delegation lets on‑premises resolvers forward queries to Route 53 Resolver endpoints and receive referrals for subdomains hosted in private hosted zones in AWS. Typical steps:\nCreate a private hosted zone for the subdomain (for example aws.healthtech.example.com) and associate it with the VPC. Create records in the private hosted zone for the applications. Create an inbound resolver delegation endpoint and record its IP addresses. On the on‑premises DNS, add an NS record for the delegated subdomain and ensure the NS name resolves to the resolver endpoint IPs. Outbound delegation (AWS → on‑premises)\nOutbound delegation enables VPC resources to resolve names hosted on on‑premises authoritative servers. Steps include creating an outbound resolver endpoint, adding an NS record in the parent zone that points to on‑premises authoritative servers, and creating a delegation rule associated with the VPC. If the authoritative server FQDN is in‑zone, create a glue A record in the private hosted zone; otherwise create a forwarding rule for the FQDN.\nWhen both parent and child zones remain on‑premises\nIf both the parent and subdomain are hosted on‑premises, you can still use outbound delegation to enable Route 53 Resolver to follow the delegation chain to the on‑premises authoritative server and return authoritative answers to VPC clients.\nBenefits\nReduces operational overhead by removing the need for customer‑managed authoritative DNS servers in many hybrid scenarios. Simplifies delegation management: a single delegation rule can cover many subdomains. Integrates into the Resolver pricing model (hourly and per‑query); there is no separate delegation charge. See the Route 53 documentation on inbound and outbound delegation for implementation details and pricing guidance.\nAuthor\nTekena Orugbani — Senior Specialist Solutions Architect at AWS, specializing in Microsoft workloads and hybrid connectivity.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get acquainted with the members of the First Cloud Journey (FCJ) program and familiarize yourself with the rules and regulations of the internship unit. Gain a solid understanding of core cloud computing concepts and the AWS service ecosystem, including Compute, Storage, Networking, Databases, and Security. Hands-on practice in creating and managing an AWS Free Tier account using best practices: setting up IAM Users, Groups, and Roles; limiting root account usage; and configuring Budgets for cost monitoring. Work with both the AWS Management Console and AWS CLI to manage resources, while building basic scripting skills for automation. Learn cost optimization strategies such as Spot Instances, the AWS Pricing Calculator, and AWS Budgets/Cost Explorer. Explore Serverless architecture through AWS Lambda and DynamoDB, and use Hugo to build and deploy static workshop websites. Study and practice essential AWS Networking components, including VPC, Subnets, Route Tables, Security Groups, NACLs, Internet Gateway, NAT Gateway, VPC Endpoints, VPC Flow Logs, VPN, Direct Connect, and Elastic Load Balancing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn to draw cloud system workflows using AWS architecture icons - Study Cloud Computing Module 01: AWS overview, policies, services and AWS Support - Create AWS Free Tier account, Admin group and Admin User - Install \u0026amp; configure AWS CLI - Learn how to request AWS Support assistance 08/11/2025 08/11/2025 https://000001.awsstudygroup.com/ https://000009.awsstudygroup.com/ 3 - Learn Markdown writing and website creation with Hugo - Complete Module 1 labs for hands-on practice - Study Spot Instances for cost optimization (~90% cheaper but interruptible) - Understand serverless concepts (Lambda, DynamoDB) - Use AWS Pricing Calculator to estimate and compare costs by Region - Review key networking services: VPC, Subnets, Route Tables, Security Groups, NACLs, VPC Peering, Transit Gateway, VPN, Direct Connect, ELB 08/12/2025 08/12/2025 https://van-hoang-kha.github.io/ 4 - Set up IAM User, Polices, Group, and Role instead of using the root account - Configure Budget/Cost Budget to monitor monthly credits - Review VPC basics: public/private subnets, Route Tables, IGW, NAT Gateway - Learn ENI (network interfaces) and Elastic IP usage - Study VPC Endpoints (Gateway vs Interface) - Compare Security Groups (stateful) and NACLs (stateless) - Enable and review VPC Flow Logs (CloudWatch / S3) - Understand VPC Peering and its common limitations - Learn Transit Gateway and how Attachments connect networks - Complete Module 02 — 01 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Direct Connect (Dedicated \u0026amp; Hosted) + Use cases and connectivity models - Understand Site-to-Site VPN with Virtual Private Gateway \u0026amp; Customer Gateway - Review Client VPN fundamentals - Study Elastic Load Balancing (ALB, NLB, CLB, Global LB) + Health checks, sticky sessions, access logs + Routing rules and target types - Complete Module 02 - 03 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Consolidate key topics: IAM, VPC, Subnets, Security, ELB, etc. + Review architecture flow and dependencies - Practice advanced VPC networking labs - Learn Route 53 Resolver for hybrid DNS setups - Configure and test VPC Peering connections 12/09/2025 14/09/2025 My Notes on AWS Services Week 1 – AWS Learning Summary 1. Core AWS Understanding Gained a clear overview of AWS and its primary service categories: Compute, Storage, Networking, Databases, Security, etc. Learned the differences between: AWS Management Console (web UI) AWS CLI (command-line access) AWS SDK (programmatic interaction) 2. Account Setup \u0026amp; Free Tier Work Created and configured an AWS Free Tier account. Installed and set up AWS CLI with: Access Key Secret Key Default Region Practiced operating AWS resources using both Console and CLI. 3. CLI Hands-On Experience Verified account identity and CLI configuration. Listed active AWS Regions. Worked with EC2: Viewed instances Generated and managed key pairs Checked running services Started writing simple automation scripts for repeated commands. 4. IAM (Identity \u0026amp; Access Management) Created and organized IAM Users, Groups, and Roles. Attached both managed and inline policies. Practiced signing in as IAM User and switching roles. Understood why daily root account usage is not recommended. 5. Cost Management \u0026amp; Optimization Learned how Spot Instances work (high savings, interruptible). Created Budgets and Free Tier usage monitoring. Used AWS Pricing Calculator to compare cost differences by region. 6. Serverless \u0026amp; Workshop Tools Learned the fundamentals of Serverless services: Lambda, DynamoDB, and more. Practiced using Hugo to build static workshop pages. 7. Networking Fundamentals Understood VPC (Virtual Private Cloud) structure. Differentiated public/private subnets and learned: Route Tables Internet Gateway (IGW) NAT Gateway Elastic IP Elastic Network Interface (ENI) Learned about: VPC Endpoints VPC Flow Logs Security Groups (stateful) vs NACLs (stateless) VPC Peering \u0026amp; Transit Gateway Intro to Site-to-Site VPN, Client VPN, Direct Connect Load Balancers: ALB, NLB, CLB, Global LB Reflection – Progress, Challenges, and Next Steps Strengths Developed Strong grasp of AWS fundamentals and how services are categorized. Confident in configuring accounts and applying IAM best practices. Improved understanding of AWS networking components. Difficulties Encountered Initially confused between CLI and SDK usage. VPC components (CIDR, subnets, NACLs, SG) were challenging at first. “EC2:Running Hours” metric missing during Budget setup due to no activity data. Estimating pricing took time because of unfamiliar service options. Advanced networking labs (Peering, Hybrid DNS, Route 53 Resolver) required multiple attempts. Improvement Plan Continue practicing CLI and networking labs. Perform more hands-on cost management tasks, including Spot Instances. Review official AWS documentation to reduce conceptual confusion. Build diagrams and summaries after complex labs to strengthen memory. "},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Explore Amazon EC2 by experimenting with different instance types, storage options (EBS, Instance Store), and deployment configurations. Evaluate AWS storage and compute services—S3, EFS, FSx, MGN, Lightsail—by testing performance and understanding practical use cases. Improve resource management skills using Tags \u0026amp; Resource Groups for organization, cost tracking, and simplified operations. Learn to monitor infrastructure and applications using Amazon CloudWatch for logging, alerts, and performance insights. Practice deploying applications using Auto Scaling Groups to evaluate system resilience and scalability. Understand AWS Transit Gateway and network connectivity patterns to design efficient multi-VPC architectures. Collaborate with team members to discuss project approaches, identify risks, and plan implementation strategies. Tasks to Carry Out This Week: Day Tasks Start date Completion date Reference Material 2 - Explore AWS Transit Gateway and its use in connecting multiple VPCs - Analyze network design scenarios where Transit Gateway improves connectivity and performance 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Experiment with Amazon EC2: launch instances, attach storage (EBS/Instance Store), and test different configurations - Evaluate S3, EFS, FSx, MGN, Lightsail for storage and lightweight compute tasks - Complete exercises in course modules 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Test deployment strategies and features of EC2 instances - Meet with team members to brainstorm project ideas and discuss potential implementation plans - Document findings and lessons learned 17/09/2025 17/09/2025 https://000004.awsstudygroup.com/ 5 - Explore resource tagging and grouping strategies to improve management and visibility - Apply Tags \u0026amp; Resource Groups to organize deployed services and monitor usage 18/09/2025 18/09/2025 https://000027.awsstudygroup.com/ 6 - Conduct a CloudWatch workshop: set up monitoring, logs, and alerts for EC2 and other services - Deploy the FCJ Management app using an Auto Scaling Group - Observe scaling behavior and collect metrics for performance analysis 19/08/2025 19/08/2025 https://000006.awsstudygroup.com/ Week 2 Achievements: Knowledge and insights gained:\nTested and understood EC2 features and storage options (EBS, Instance Store) in different deployment scenarios Evaluated S3, EFS, FSx, MGN, and Lightsail for performance, scalability, and practical application Improved resource organization using Tags \u0026amp; Resource Groups Learned how CloudWatch can track system performance, generate alerts, and analyze logs Gained experience designing multi-VPC networks using AWS Transit Gateway Applied Auto Scaling Groups to deploy resilient applications Collaborated effectively with team members to brainstorm and plan project features Hands-on outcomes:\nLaunched and tested EC2 instances in different configurations Deployed applications using Auto Scaling and monitored performance with CloudWatch Organized and managed AWS resources using Tags \u0026amp; Resource Groups Explored various AWS storage services for suitability in different scenarios Documented lessons learned and refined project implementation plans with the team Combined theoretical understanding with hands-on experimentation to gain practical insights into AWS service capabilities and architectural design.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Get practical experience with AWS storage services, including: Amazon S3, Storage Gateway, AWS Backup, Amazon FSx. Strengthen knowledge of AWS security and access management concepts through IAM (Users, Groups, Roles, Policies). Gain hands-on experience operating services via both Console and CLI. Record observations and structure notes to support upcoming exercises. Tasks for This Week: Day Tasks Start date Completion date Reference Material 2 - Investigate AWS storage options: S3, Snow Family devices, Storage Gateway, and AWS Backup; explore their roles in data protection and recovery 22/09/2025 22/09/2025 FCJ YT, Cloud Journey 3 - Create and configure Amazon S3 buckets; set up AWS Backup plans and test backup/restore workflows 23/09/2025 23/09/2025 My Notes on AWS Services 4 - Deploy a File Storage Gateway to enable hybrid access to cloud storage from on-premises systems 24/09/2025 24/09/2025 My Notes on AWS Services 5 - Launch Amazon FSx for Windows File Server and verify file-sharing functionality and AD integration 25/09/2025 25/09/2025 My Notes on AWS Services 6 - Study AWS IAM and security mechanisms; document users, roles, policies, and recommended best practices 26/09/2025 26/09/2025 Video Security Week 3 Achievements: Amazon S3 Object storage that scales virtually without limits; supports individual objects up to 5 TB. Supports multipart uploads, notifications, and automated lifecycle management. Guarantees 99.999999999% durability and 99.99% availability. Offers multiple storage classes: Standard, Standard-IA, One Zone-IA, Intelligent-Tiering, Glacier, Deep Archive. Lifecycle rules can move data automatically between tiers. Can host static websites and supports CORS for cross-origin requests. Access controlled via ACLs, bucket policies, and IAM policies; private access possible with VPC endpoints. Versioning enables recovery from accidental deletion or overwrites. Glacier provides cost-effective long-term storage with three retrieval speeds: Expedited, Standard, Bulk. Snow Family \u0026amp; Storage Gateway Snowball: ~80 TB appliance for transferring on-premises data to AWS. Snowball Edge: ~100 TB storage plus compute capabilities at the edge for pre-processing data. Snowmobile: truck-scale device (~100 PB) for extremely large datasets. Storage Gateway: bridges on-premises systems with AWS storage: File Gateway (NFS/SMB): presents S3 as file shares. Volume Gateway (iSCSI): block-level storage with snapshots to EBS/S3. Tape Gateway (VTL): virtual tape library storing to S3/Glacier, replacing physical tapes. Backup \u0026amp; Disaster Recovery RTO (Recovery Time Objective): maximum allowable downtime. RPO (Recovery Point Objective): maximum acceptable data loss period. Common recovery strategies: Backup \u0026amp; Restore, Pilot Light, Low-capacity Active-Active, Full Active-Active. AWS Backup: centralized service to manage and monitor backups for EBS, EC2, RDS, DynamoDB, EFS, and Storage Gateway. FSx Configured Amazon FSx for Windows File Server. Provides managed Windows-native file storage accessible through SMB. Ideal for workloads requiring Active Directory integration or Windows-based applications. Security \u0026amp; IAM Shared Responsibility Model: AWS secures infrastructure; customers secure their data and configurations. Root Account: full permissions — best practice is to minimize use, create IAM admins, and secure credentials. IAM User: identity with no default permissions, can be grouped for easier management. IAM Policy: JSON-based permission rules; types include identity-based and resource-based; deny takes priority over allow. IAM Role: temporary credentials via STS, combining permissions and a trust policy. Useful for least-privilege access, cross-account access, and service-level permissions. "},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Strengthen hands-on knowledge of AWS Identity \u0026amp; Security:\nExplore IAM, Cognito, AWS Identity Center (SSO), and Organizations for access control and governance. Practice AWS KMS to protect data at rest. Work with AWS Security Hub to aggregate security information and evaluate compliance standards.\nUnderstand advanced IAM mechanisms, including Roles, Condition Keys, and Permission Boundaries, to control and limit access.\nAnalyze compute options and optimize costs by comparing EC2 and Lambda for different use cases.\nImprove skills in reading, analyzing, and translating AWS technical documentation for internal knowledge sharing.\nTasks for This Week: Day Tasks Start Date Completion Date Reference Material 2 - Explore AWS Identity \u0026amp; Security services: + Experiment with Amazon Cognito: User Pools (signup/sign-in) and Identity Pools (access to other AWS services). + Study AWS Organizations: manage multi-account setups, Organizational Units (OUs), Service Control Policies (SCPs), and consolidated billing. + Understand AWS Identity Center (SSO) for cross-account and external application access. + Practice AWS KMS: encrypt data at rest using CMKs and Data Keys. - Hands-on labs: IAM Roles, Permission Boundaries, resource tagging, Security Hub, KMS workflows. 29/09/2025 29/09/2025 AWS Study Group YouTube Playlist 3 - Explore AWS Security Hub: enable the service, integrate with GuardDuty, Config, and Inspector, review compliance frameworks, analyze findings, and respond to alerts. - Compare EC2 vs Lambda costs across usage patterns and determine the most cost-effective solution. - Apply tag-based IAM policies to control EC2 access and validate effectiveness. 30/09/2025 30/09/2025 AWS Security Hub Overview EC2 vs Lambda Cost Optimization 4 - Study IAM Roles and Condition Keys: attach roles to EC2/Lambda, differentiate Trust Policies vs Permission Policies, and practice conditional access based on IP, region, or tags. - Practice encrypting data at rest using AWS KMS and compare with encryption in transit. 01/10/2025 01/10/2025 IAM Role Documentation AWS KMS Overview 5 - Learn Permission Boundaries: define maximum permission limits for users or roles, compare with standard IAM policies. - Lab: create a user with a permission boundary restricting EC2 creation to a specific region; verify effectiveness via CLI and Console. 02/10/2025 02/10/2025 IAM Permission Boundaries 6 - Explore AWS monitoring and auditing services: + Review AWS CloudTrail to track API activity across AWS accounts. + Use AWS Config to observe resource compliance and configuration changes. + Use Amazon CloudWatch to monitor metrics, create dashboards, and set alarms for key resources. - Practice creating alerts and analyzing logs to understand system activity and potential security issues. 03/10/2025 03/10/2025 AWS CloudTrail Overview AWS Config Overview Amazon CloudWatch Overview Week 4 Achievements: Improved knowledge and hands-on skills with AWS Identity \u0026amp; Security:\nConfigured Cognito for user authentication and authorization. Managed multi-account setups with AWS Organizations, using OUs, SCPs, and consolidated billing. Configured AWS Identity Center (SSO) for cross-account and external application access. Applied AWS KMS for encrypting data at rest. Monitored security posture with AWS Security Hub and responded to findings. Used IAM Permission Boundaries to limit maximum permissions for users and roles. Completed labs reinforcing key concepts: IAM Roles \u0026amp; Conditions, Permission Boundaries, Security Hub, tag-based EC2 access control, KMS encryption.\nApplied advanced IAM concepts: Trust Policies, Permission Policies, and Condition Keys to implement fine-grained access control.\nAnalyzed cost-effectiveness of compute services, determining appropriate workloads for EC2 vs Lambda.\nStrengthened ability to read, synthesize, and present AWS technical information for internal knowledge sharing.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Develop a solid understanding of core database principles and the distinctions between OLTP and OLAP workloads. Explore and gain hands-on experience with AWS database solutions: Amazon RDS, Amazon Redshift, and Amazon ElastiCache. Enhance SQL proficiency, moving from fundamental commands to more advanced query techniques. Learn best practices for deploying, managing, and optimizing AWS-managed databases. Bridge theoretical database knowledge with practical implementation in the cloud. Tasks for This Week: Day Tasks Start Date Completion Date Reference Material 2 - Review foundational database concepts: + Core components: Database, Session, Primary/Foreign Keys + Indexing, Partitioning, Execution Plans, Buffer, Logs - Compare RDBMS vs NoSQL systems - Understand OLTP vs OLAP and their practical applications 06/10/2025 06/10/2025 YouTube - Database Concepts 3 - Explore Amazon Redshift and Amazon ElastiCache: + Redshift: columnar storage, MPP architecture, leader \u0026amp; compute nodes, Redshift Spectrum, transient clusters, cost optimization + ElastiCache: Redis \u0026amp; Memcached, caching patterns, auto-failover - Hands-on labs: Lab 5 – RDS, Lab 43 – DMS \u0026amp; SCT (testing migration workflows) 07/10/2025 07/10/2025 W3Schools SQL Amazon Redshift Docs Database Internals (book) 4 - Reinforce basic SQL skills: + SELECT, INSERT, UPDATE, DELETE + JOIN, GROUP BY, ORDER BY + Execute queries in sandbox/test environment for practice 08/10/2025 08/10/2025 W3Schools SQL 5 - Advance SQL practice: + Subqueries, Aggregate Functions, and Constraints + Explore query optimization techniques and effective use of primary/foreign keys 09/10/2025 09/10/2025 W3Schools SQL 6 - Apply knowledge in Amazon RDS: + Launch and configure RDS instances + Connect to databases, execute queries, and manage data + Perform backup, snapshot, and restore operations - Consolidate understanding of AWS database services and how they integrate in real scenarios 10/10/2025 10/10/2025 Amazon RDS Doc Week 5 Achievements: Solid understanding of key database fundamentals:\nCore concepts: Database, Session, Index, Partition, Execution Plan, Buffer, Logs Difference between RDBMS and NoSQL Practical distinction between OLTP and OLAP Gained hands-on experience with AWS database services:\nAmazon RDS – managed relational database service Amazon Redshift – columnar MPP data warehouse for OLAP workloads Amazon ElastiCache – caching layer to reduce database load Successfully completed lab exercises:\nLab 5 – Amazon RDS: instance creation, connectivity, database operations, backup/restore Redshift lab: explored data organization and columnar storage Lab 43 – DMS \u0026amp; SCT: attempted migration workflows and troubleshooting Advanced SQL skills:\nCRUD operations: SELECT, INSERT, UPDATE, DELETE Complex queries: JOIN, GROUP BY, ORDER BY, Subqueries, Aggregate Functions, Constraints Query optimization and key usage Developed the ability to connect theoretical knowledge with practical cloud implementations, planning database architecture using RDS, Redshift, and ElastiCache.\nEnhanced capability to synthesize and apply database concepts in real-world AWS projects.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Continue exploring the internship environment and familiarize with AWS learning resources. Begin the AWS Fundamentals Specialization on Coursera to strengthen foundational cloud knowledge. Learn how to design AWS architectures using draw.io and review sample reference architectures (RAG Chatbot, AI Agents) from AWS official guidance. Develop the project\u0026rsquo;s initial architecture draft using the Amazon Q CLI, preparing for hands-on deployment. Tasks for This Week: Day Tasks Start Date Completion Date Reference Material 2 - Access and navigate the AWS Fundamentals Specialization on Coursera. 12/10/2025 12/10/2025 Coursera 3 - Watch introductory AWS video tutorials on YouTube - Practice drawing AWS architecture diagrams using draw.io - Study the blog: Conversational Chatbots using RAG on AWS 13/10/2025 13/10/2025 YouTube, AWS Blog Video Series 4 - Review AI Agents reference architecture - Experiment with deploying a local SQL Server using RDS 14/10/2025 14/10/2025 AWS Solutions Library Advanced Multimodal Chatbot with Speech-to-Speech on AWS 5 - Attend the \u0026ldquo;Data Science on AWS\u0026rdquo; workshop + Take notes on AWS data analytics, ML pipelines, and relevant services 15/10/2025 15/10/2025 FPT Workshop - Data Science on AWS 6 - Build the first draft of the project architecture using Amazon Cost calculator - Save and document the diagram on Notion for internal team feedback 16/10/2025 16/10/2025 Week 6 Achievements: Learned how to create and visually represent AWS architectures using draw.io. Gained understanding of RAG-based chatbot structures and AI Agent architectures. Practiced connecting a local SQL Server to AWS RDS, preparing for production deployment. Expanded knowledge of data analytics and ML pipelines through participation in the \u0026ldquo;Data Science on AWS\u0026rdquo; workshop. Completed the initial project architecture using Amazon Q CLI and stored the design in Notion for internal review. "},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Goals: Organize and consolidate knowledge acquired in the first half of the internship. Deepen understanding of Security and Resiliency in AWS. Build a strong foundation for the Midterm Exam. Tasks for This Week: Day Task Start Date Completion Date Reference 2 Midterm Review:\nPart 1: Security Fundamentals\n- IAM (Identity \u0026amp; Access Management)\n- KMS (Key Management Service)\n- Security Groups \u0026amp; NACLs\n- Secrets Manager\n- GuardDuty (Threat Detection)\n- Shield \u0026amp; WAF (DDoS and Web Application Protection)\nPart 2: Resilient Architecture\n- Core metrics: RTO/RPO\n- Multi-AZ RDS\n- Disaster Recovery Strategies\n- Auto Scaling \u0026amp; Load Balancing\n- Route 53 \u0026amp; DNS\n- AWS Backup 20/10/2025 20/10/2025 Review Link 3 Self-study: Systematize security services knowledge and practice simulation labs 21/10/2025 21/10/2025 4 Self-study: Redraw High Availability (HA) and Disaster Recovery (DR) architecture diagrams to reinforce concepts 22/10/2025 22/10/2025 5 Take practice tests to review knowledge and identify weak areas 23/10/2025 23/10/2025 Practice Exams 6 Compile questions and summarize key notes for final review 24/10/2025 24/10/2025 Additional Resources Week 7 Achievements: Strengthening Security Awareness and Resilient Architecture Defense in Depth Mindset:\nClearly distinguished between Security Groups (stateful, instance-level) and NACLs (stateless, subnet-level). Understood protection at multiple layers: WAF for application layer (L7), Shield for infrastructure (L3/L4), and GuardDuty for intelligent threat detection. Mastered the role of KMS and Secrets Manager in safeguarding sensitive data at rest and in transit. Resiliency \u0026amp; Recovery:\nLearned and internalized RTO (Recovery Time Objective) and RPO (Recovery Point Objective) for selecting appropriate DR strategies (Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-site Active/Active). Gained a clear understanding of Multi-AZ RDS for High Availability versus Read Replicas for scaling read operations. Understood how Route 53, ELB, and Auto Scaling work together to build flexible, self-healing, and scalable architectures. "},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Goals: Consolidate knowledge from previous weeks into a structured understanding. Reinforce key architectural patterns emphasizing High Performance and Cost Optimization. Prepare for and complete the Midterm Examination with confidence. Focus Areas This Week: Day Focus Start Date Completion Date Reference 2 Review High-Performance \u0026amp; Cost-Optimized Architectures:\n- Explore Serverless Containers (AWS Fargate) and Event-Driven Compute (AWS Lambda), identifying optimal use cases.\n- Study Auto Scaling triggered by CloudWatch metrics.\n- Understand AWS Cost Explorer, S3 Storage Tiering, and Lifecycle Management for cost efficiency.\n- Review full VPC architecture including NAT Gateway design. 27/10/2025 27/10/2025 Review Link 3 Knowledge Consolidation: Systematize all concepts covered so far, ensuring connections between performance, security, and cost considerations. 28/10/2025 28/10/2025 4 Practice \u0026amp; Test Application: Apply learned concepts through practice questions and scenario-based exercises to identify gaps and strengthen understanding. 29/10/2025 29/10/2025 Practice Exams 5 Final Preparation: Review key notes, diagrams, and study materials to consolidate understanding before the exam. 30/10/2025 30/10/2025 6 Midterm Examination: Demonstrate ability to design and evaluate AWS architectures balancing performance, security, and cost-effectiveness. 31/10/2025 31/10/2025 Week 8 Achievements: High-Performance Architecture Understanding:\nGained clarity on when to use AWS Fargate vs. AWS Lambda based on workload patterns. Applied Auto Scaling principles using CloudWatch metrics for maintaining stability under fluctuating demand. Cost Optimization Skills:\nDeveloped proficiency in AWS Cost Explorer for analyzing usage patterns and optimizing costs. Applied S3 Storage Tiering and Lifecycle policies to automate cost-efficient storage management. Reviewed networking costs in depth, including the impact of NAT Gateways within a VPC. Exam Readiness \u0026amp; Validation:\nSuccessfully completed the Midterm Exam, confirming understanding of high-performance, cost-optimized, and secure AWS architectures. Identified strengths in networking and highlighted areas for deeper practice in serverless architecture orchestration. "},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Goals: Progress the development of a personalized recommendation system. Continuously refine data inputs and address issues emerging during the training cycle. Focus Areas This Week: Day Focus Start Date Completion Date Reference 2 Initial Model Setup:\n- Configure the algorithm framework\n- Load initial datasets for processing\n- Start preliminary training routines 10/11/2025 11/11/2025 AWS Personalize Docs https://docs.aws.amazon.com/personalize/ 3 Review early outputs and investigate anomalies or unexpected behaviors 12/11/2025 12/11/2025 4 Data Refinement:\n- Introduce additional data fields\n- Clean and reconcile dataset inconsistencies\n- Repeat training cycle with revised data 12/11/2025 13/11/2025 5 Assess updated model performance and examine metric shifts 14/11/2025 14/11/2025 6 Iterative observations and planning next refinement steps Week 9 Reflection: Navigating Data and Training Challenges This week highlighted the practical challenges of building a recommendation system and working with constrained data environments:\nLimited Interaction Signals:\nThe dataset mainly tracks completed actions (\u0026ldquo;Bookings\u0026rdquo;), lacking intermediary signals like \u0026ldquo;Views\u0026rdquo; or \u0026ldquo;Clicks\u0026rdquo;. This scarcity constrains the system’s ability to infer deeper user preferences, reflected in modest evaluation scores.\nValidation Constraints:\nWithout a fully functional front-end interface, real-world validation remains largely theoretical. Current assessment relies on quantitative metrics rather than intuitive, visual confirmation.\nPipeline Sensitivity to Schema Changes:\nIntroducing new data attributes requires revisiting the preprocessing pipeline. Each adjustment involves re-cleaning, re-mapping, and retraining, highlighting the delicacy of the end-to-end workflow.\nKey Takeaways:\nWhile specific cloud tools support dataset handling and model training, the main insights are procedural: ensuring dataset completeness, maintaining flexible pipelines, and carefully managing iterative training cycles.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Below is a week-by-week summary of activities and links to the detailed worklog pages.\nWeek 1: Getting familiar with AWS, account setup, CLI and EC2 hands‑on\nWeek 2: Explored EC2, storage options (EBS, S3, EFS), CloudWatch and Transit Gateway\nWeek 3: Deep dive into storage services (S3, Storage Gateway, FSx) and strengthened IAM knowledge\nWeek 4: Identity \u0026amp; Security — Cognito, KMS, Security Hub, IAM best practices\nWeek 5: Databases on AWS — RDS, Redshift, ElastiCache and advanced SQL practice\nWeek 6: Architecture design and project planning; Amazon Q CLI and draft architecture\nWeek 7: Midterm review — Security fundamentals, resiliency, HA/DR patterns\nWeek 8: Consolidation and midterm exam preparation; cost and performance optimizations\nWeek 9: Recommendation system experiments and iterative model training (AWS Personalize use cases)\nWeek 10: Solution architecture, IaC (CloudFormation/Terraform) and VPC/network foundations\nWeek 11: Operational readiness — RDS multi‑AZ, CI/CD pipelines, automated deployments\nWeek 12: Project finalization — monitoring, security hardening, cost optimization and handover\n"},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Create Amazon RDS","tags":[],"description":"","content":"Step 1: Create RDS MySQL Instance Go to RDS Console → Databases → Create database\nChoose database creation method:\nStandard create Engine options:\nEngine type: MySQL Engine version: MySQL 8.0.35 (or latest 8.0.x) Templates:\nFree tier (for workshop/testing) Settings:\nDB instance identifier: daivietblood-db Master username: admin Credentials management: Self managed Master password: YourSecurePassword123! Confirm password: YourSecurePassword123! ⚠️ Important: Save your password securely. You will need it to connect from Lambda.\nStep 2: Instance Configuration Instance configuration: DB instance class: db.t3.micro (Free tier eligible) Storage type: General Purpose SSD (gp2) Allocated storage: 20 GiB Storage autoscaling: Disable (for cost control) Step 3: Connectivity Connectivity:\nCompute resource: Don\u0026rsquo;t connect to an EC2 compute resource Network type: IPv4 Virtual private cloud (VPC): daivietblood-vpc DB subnet group: daivietblood-db-subnet-group Public access: No ⚠️ Important! VPC security group: Choose existing Existing VPC security groups: daivietblood-rds-sg Availability Zone: ap-southeast-1a Database port:\nDatabase port: 3306 Step 4: Database Authentication Database authentication: Password authentication Step 5: Additional Configuration Database options:\nInitial database name: daivietblood DB parameter group: default.mysql8.0 Option group: default:mysql-8-0 Backup:\nEnable automated backups: Yes Backup retention period: 7 days Backup window: No preference Encryption:\nEnable encryption: Yes (default) Monitoring:\nEnable Enhanced monitoring: No (to reduce cost) Maintenance:\nEnable auto minor version upgrade: Yes Maintenance window: No preference Deletion protection:\nEnable deletion protection: No (for workshop) Click Create database\nℹ️ RDS creation takes 10-15 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 6: Get RDS Endpoint After RDS is available:\nGo to RDS Console → Databases → Click daivietblood-db\nIn Connectivity \u0026amp; security tab, copy:\nEndpoint: daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com Port: 3306 Save these values for Lambda configuration:\nDB_HOST=daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com DB_PORT=3306 DB_NAME=daivietblood DB_USER=admin DB_PASSWORD=YourSecurePassword123! Step 7: Create Database Schema Since RDS is in Private Subnet, you need to connect via a bastion host or use Lambda to initialize the schema.\nOption A: Using Lambda to Initialize (Recommended)\nCreate a one-time Lambda function to initialize the database:\n// init-db.js const mysql = require(\u0026#39;mysql2/promise\u0026#39;); exports.handler = async (event) =\u0026gt; { const connection = await mysql.createConnection({ host: process.env.DB_HOST, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); // Create tables const createUsersTable = ` CREATE TABLE IF NOT EXISTS users ( id INT AUTO_INCREMENT PRIMARY KEY, email VARCHAR(255) UNIQUE NOT NULL, name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, phone VARCHAR(20), created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; const createDonationsTable = ` CREATE TABLE IF NOT EXISTS donations ( id INT AUTO_INCREMENT PRIMARY KEY, user_id INT NOT NULL, donation_date DATE NOT NULL, location VARCHAR(255), status ENUM(\u0026#39;scheduled\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;scheduled\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (user_id) REFERENCES users(id) ) `; const createEmergencyRequestsTable = ` CREATE TABLE IF NOT EXISTS emergency_requests ( id INT AUTO_INCREMENT PRIMARY KEY, requester_name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, units_needed INT NOT NULL, hospital VARCHAR(255) NOT NULL, urgency ENUM(\u0026#39;critical\u0026#39;, \u0026#39;urgent\u0026#39;, \u0026#39;normal\u0026#39;) DEFAULT \u0026#39;normal\u0026#39;, status ENUM(\u0026#39;open\u0026#39;, \u0026#39;fulfilled\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;open\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; await connection.execute(createUsersTable); await connection.execute(createDonationsTable); await connection.execute(createEmergencyRequestsTable); await connection.end(); return { statusCode: 200, body: JSON.stringify({ message: \u0026#39;Database initialized successfully\u0026#39; }) }; }; Verification Checklist RDS instance created and status is \u0026ldquo;Available\u0026rdquo; RDS is in Private Subnet (Public access: No) RDS Security Group only allows access from Lambda SG Endpoint and credentials saved securely Initial database daivietblood created Database schema initialized (tables created) Troubleshooting Issue Solution Cannot connect to RDS Verify Security Group allows inbound from Lambda SG RDS creation failed Check Service Quotas for RDS instances Connection timeout Ensure Lambda is in same VPC and has NAT Gateway access "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create API Gateway","tags":[],"description":"","content":"Step 1: Create REST API Go to API Gateway Console → Create API\nChoose API type:\nREST API → Build Create new API:\nProtocol: REST Create new API: New API API name: daivietblood-api Description: REST API for DaiVietBlood system Endpoint Type: Regional Click Create API\nStep 2: Create Resources 2.1. Create /users Resource\nSelect root / → Actions → Create Resource\nConfigure:\nResource Name: users Resource Path: users Enable API Gateway CORS: ✅ Check Click Create Resource\n2.2. Create /emergency-requests Resource\nSelect root / → Actions → Create Resource\nConfigure:\nResource Name: emergency-requests Resource Path: emergency-requests Enable API Gateway CORS: ✅ Check Click Create Resource\nStep 3: Create Methods for /users 3.1. GET /users\nSelect /users → Actions → Create Method → GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-get-users Click Save → OK (to add permission)\n3.2. POST /users\nSelect /users → Actions → Create Method → POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-create-user Click Save → OK\nStep 4: Create Methods for /emergency-requests 4.1. GET /emergency-requests\nSelect /emergency-requests → Actions → Create Method → GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Function: daivietblood-emergency-requests Click Save → OK\n4.2. POST /emergency-requests\nSelect /emergency-requests → Actions → Create Method → POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ✅ Check Lambda Function: daivietblood-emergency-requests Click Save → OK\nStep 5: Enable CORS For each resource (/users, /emergency-requests):\nSelect resource → Actions → Enable CORS\nConfigure:\nAccess-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Amz-Date, Authorization, X-Api-Key Access-Control-Allow-Origin: * Click Enable CORS and replace existing CORS headers\nClick Yes, replace existing values\nStep 6: Deploy API Actions → Deploy API\nDeployment stage:\nDeployment stage: [New Stage] Stage name: prod Stage description: Production stage Click Deploy\nCopy the Invoke URL:\nhttps://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod ℹ️ Save this URL. You will need it for frontend configuration.\nStep 7: API Structure Summary After completing, your API structure should look like:\ndaivietblood-api │ ├── /users │ ├── GET → daivietblood-get-users │ ├── POST → daivietblood-create-user │ └── OPTIONS (CORS) │ └── /emergency-requests ├── GET → daivietblood-emergency-requests ├── POST → daivietblood-emergency-requests └── OPTIONS (CORS) Verification Checklist REST API created /users resource created with GET and POST methods /emergency-requests resource created with GET and POST methods CORS enabled for all resources API deployed to prod stage Invoke URL saved "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.2-prerequiste/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Before starting this workshop, ensure you have:\n1. AWS Account\nActive AWS Account with Administrator access Recommended: Use IAM User instead of Root account Region: Asia Pacific (Singapore) - ap-southeast-1 2. Local Development Tools\nTool Version Purpose Node.js \u0026gt;= 18.x Run Lambda functions locally npm/yarn Latest Package management AWS CLI \u0026gt;= 2.x Interact with AWS services Git Latest Version control 3. Knowledge Requirements\nBasic understanding of AWS services (VPC, EC2, S3) Familiarity with REST APIs Basic Node.js/JavaScript or Python Basic React knowledge Step 1: Configure AWS CLI Install AWS CLI from AWS CLI Installation Guide\nConfigure credentials:\naws configure Enter your credentials: AWS Access Key ID: [Your Access Key] AWS Secret Access Key: [Your Secret Key] Default region name: ap-southeast-1 Default output format: json Verify configuration: aws sts get-caller-identity Step 2: Create IAM User for Workshop Go to IAM Console → Users → Create user\nUser details:\nUser name: workshop-admin Select: Provide user access to the AWS Management Console Set permissions:\nSelect: Attach policies directly Search and select: AdministratorAccess Create user and save credentials securely\n⚠️ Security Note: After completing the workshop, delete this IAM user or remove AdministratorAccess policy.\nStep 3: Verify Service Quotas Ensure your account has sufficient quotas for:\nService Resource Minimum Required VPC VPCs per Region 1 VPC Subnets per VPC 4 VPC NAT Gateways per AZ 1 RDS DB Instances 1 Lambda Concurrent Executions 10 API Gateway REST APIs 1 S3 Buckets 2 Check quotas at: Service Quotas Console → Select service → View quotas\nStep 4: Prepare Source Code Clone the sample repository: git clone https://github.com/your-repo/daivietblood-workshop.git cd daivietblood-workshop Project structure: daivietblood-workshop/ ├── frontend/ # React application │ ├── src/ │ └── package.json ├── backend/ # Lambda functions │ ├── functions/ │ └── package.json ├── infrastructure/ # CloudFormation templates │ └── templates/ └── README.md Install dependencies: # Frontend cd frontend \u0026amp;\u0026amp; npm install # Backend cd ../backend \u0026amp;\u0026amp; npm install Step 5: Cost Estimation Service Configuration Est. Cost/Day NAT Gateway 1 NAT Gateway ~$1.08 RDS db.t3.micro ~$0.52 Lambda Free Tier $0.00 API Gateway Free Tier $0.00 S3 \u0026lt; 5GB ~$0.01 CloudFront \u0026lt; 1GB transfer ~$0.01 Amplify Build \u0026amp; Host ~$0.50 Total estimated: ~$2-3/day\n💡 Tip: Complete the workshop in 1-2 days and clean up resources immediately to minimize costs.\nChecklist Before Starting AWS Account ready with Administrator access AWS CLI installed and configured Node.js \u0026gt;= 18.x installed Git installed Source code cloned Region set to ap-southeast-1 "},{"uri":"https://docvoxx.github.io/internship-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Dai Viet Blood Donation \u0026amp; Emergency System (DaiVietBlood) Implemented by: Skyline Team – FPT University Ho Chi Minh City\nDate: December 7, 2025\nDownload PDF\nTABLE OF CONTENTS BACKGROUND AND MOTIVATION 1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations ACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN IMPLEMENTATION TEAM RESOURCES \u0026amp; ESTIMATED PERSONNEL COSTS ACCEPTANCE 1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Customer Background:\nThe DaiVietBlood system is designed to serve the community, including voluntary blood donors, patients in need of emergency blood, and healthcare professionals in Vietnam. The primary customers are blood donors, patient families, and medical staff responsible for managing blood inventory and donation schedules. They require a centralized, reliable platform to optimize the matching process between donors and recipients and improve communication during emergencies. In the context of digital health transformation, DaiVietBlood provides a secure, accessible solution to address localized blood shortages.\nBusiness and Technical Objectives:\nMigrating the DaiVietBlood system from a local/on-premise environment to AWS offers superior advantages:\nBusiness: AWS allows the application to scale flexibly according to the user base, reduces hardware infrastructure operational costs, and ensures consistent performance nationwide. Technical: AWS provides High Availability and medical data security. Adopting a Serverless Architecture (AWS Lambda, API Gateway, Cognito, RDS) simplifies backend management, accelerates development, and reduces maintenance costs. The system integrates comprehensive monitoring (CloudWatch) and adheres to strict security standards. Summary of Key Use Cases:\nRole Key Function Short Description Guest Access Public Information View donation guidelines, blood compatibility charts, and educational articles without logging in. Member Register/Login, Profile Management Create accounts, update personal information and blood type. Book Blood Donation Select time slots and locations for donation. Submit Emergency Request Submit urgent blood requests; the system automatically finds suitable donors. Staff Manage Requests \u0026amp; Inventory Approve emergency requests, confirm donation schedules, update blood stock. Administrator (Admin) System Administration Manage user accounts, configure donation slots, view overview reports. Summary of Partner’s Professional Services: The Skyline Team will provide comprehensive digital transformation services, including assessing the current local application, designing a Cloud-native architecture, and executing the migration of the system to an AWS Serverless environment. We commit to delivering a secure, scalable system accompanied by automated CI/CD pipelines and detailed operational documentation.\n1.2 PROJECT SUCCESS CRITERIA Functionality: 100% of core functions (registration, scheduling, emergency requests, administration) operate stably on AWS with no regression errors. Availability: System achieves Uptime ≥ 99.9%, ensuring continuous 24/7 access. Performance: Application response time improves by at least 30% compared to the local version. Emergency request processing time is reduced by 40%. Cost: Infrastructure costs are optimized by at least 20% thanks to the Serverless model and Auto-scaling. User Experience: UAT acceptance rate reaches a minimum of 95% for all user roles. Security: Full compliance with data encryption, access management (IAM), and API security requirements. Operations: CI/CD pipeline is fully automated with deployment time \u0026lt; 10 minutes. Monitoring system covers 100% of critical services. 1.3 ASSUMPTIONS Technical \u0026amp; Architectural Assumptions:\nSource Code: The current Local application (Frontend \u0026amp; Backend) is functionally complete. The project focuses on Refactoring for the Cloud (Serverless), excluding the development of new features. AWS Region: The entire infrastructure is deployed in Singapore (ap-southeast-1) to optimize latency for users in Vietnam. Note: During the testing phase, due to limited VPC configurations and Free Tier/Student resources, latency may fluctuate (estimated ~3.5s/request). Service Limits: The AWS account uses default limits (Soft limits). Increasing limits to reduce latency will be approved by the Customer when necessary. Third-party Integration: The system uses the Gemini API for AI support features. Access Rights: The Skyline Team is granted Admin access (IAM Role) to provision resources. Operational \u0026amp; Financial Assumptions:\nDomain: The Customer owns the domain name (e.g., daivietblood.com) and DNS configuration rights. Cost: The cost estimate is based on an assumption of approximately 50,000 API requests/month. Actual costs depend on usage levels (Pay-as-you-go). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The DaiVietBlood system utilizes a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nKey Components:\nNetwork Infrastructure (VPC): Public Subnet: Contains Internet Gateway and NAT Gateway. Private Subnet: Contains AWS Lambda and Amazon RDS to isolate and secure data, preventing direct Internet access. Application \u0026amp; Data: Frontend: Hosted on AWS Amplify, distributed via Amazon CloudFront (CDN), and assets stored on S3. Authentication: Amazon Cognito manages identity and issues JWT tokens. API \u0026amp; Compute: Amazon API Gateway receives requests and routes them to AWS Lambda for business logic processing. Database: Amazon RDS stores structured data, located in the Private Subnet. DevOps \u0026amp; Monitoring: CI/CD: Uses AWS CodePipeline, CodeBuild, CodeDeploy to automate the deployment process. Monitoring: Amazon CloudWatch centrally collects logs and metrics. 2.2 TECHNICAL PLAN The technical implementation process follows the Infrastructure-as-Code (IaC) methodology:\nInfrastructure Automation: Use AWS CloudFormation to provision VPC, Lambda, RDS, and API Gateway, ensuring consistency across environments (Dev/Staging/Prod). Application Development: Refactor backend into modular Lambda functions (NodeJS/Python). Environment variables and sensitive information (DB credentials) are securely encrypted. CI/CD Process: Source (GitHub) -\u0026gt; Build (CodeBuild) -\u0026gt; Deploy (CloudFormation/CodeDeploy). Includes a Manual Approval step before deploying to the Production environment. Testing Strategy: Unit Tests for Lambda, Integration Tests for API, and Load Tests to ensure capacity. 2.3 PROJECT PLAN The project applies the Agile Scrum model over 8 weeks (4 Sprints):\nSprint 1 (Foundation): Set up AWS Account, VPC, RDS. Sprint 2 (Backend Core): Develop Lambda, API Gateway, Cognito. Sprint 3 (Integration): Deploy Frontend (Amplify), finalize CI/CD Pipeline. Sprint 4 (Stabilization): UAT, Performance Optimization, Handover. 2.4 SECURITY CONSIDERATIONS Access Management: Use Cognito for user authentication and IAM Roles for service authorization (Least Privilege). Network Isolation: Database and Lambda are located in the Private Subnet, accessing the Internet only via NAT Gateway. Data Protection: Data encryption At-rest (on RDS/S3) and In-transit (via HTTPS). Security Monitoring: CloudWatch Logs record all activities for auditing and intrusion detection. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Phase Timeline Key Activities Deliverables Estimate (Man-days) Analysis \u0026amp; Design Week 1 Assess Local state, design Cloud architecture, plan migration. SRS Document, Architecture Diagram, API Specs. 5 Local Development Week 2-3 Build backend logic, database schema, local unit tests. Backend Prototype, Database Schema. 10 Frontend \u0026amp; Integration Week 4-5 Develop Frontend, integrate local APIs, prepare code for refactoring. Completed Local Application. 10 AWS Infrastructure Setup Week 6 Write CloudFormation scripts, provision VPC, RDS, IAM. IaC Templates, Secure VPC Environment. 5 Refactor \u0026amp; Deploy Backend Week 7-8 Convert to Lambda, configure API Gateway, Cognito. Serverless Backend active on AWS. 10 Deploy Frontend \u0026amp; CI/CD Week 9-10 Host Frontend on Amplify, set up automated Pipeline. Production URL, CI/CD Pipeline. 10 Testing \u0026amp; Go-live Week 11 UAT, Security Testing, Performance Optimization. UAT Report, Security Report. 5 Handover \u0026amp; Training Week 12 Transfer accounts, operations training, handover documentation. Operations Manual, Acceptance. 5 3.2 OUT OF SCOPE Optimal user search algorithm based on real-time Geo-location (currently using simplified logic). Complex Auto-scaling for the Database layer (currently using basic RDS). Deep Latency Optimization for regions outside Singapore. Advanced Security Compliance standards such as HIPAA/PCI-DSS. 3.3 PATH TO PRODUCTION To upgrade from the current MVP to a large-scale Production system, the following are required:\nEnvironment Strategy: Strictly separate Dev/Staging/Prod environments across different AWS accounts (Multi-account strategy). Database Scaling: Migrate to Amazon Aurora Serverless or use Read Replicas to increase read/write capacity. Enhanced Monitoring: Integrate AWS X-Ray to trace requests and identify performance bottlenecks. Strengthened Security: Deploy AWS WAF with rules to block DDoS and automated bots; use Amazon Inspector for periodic vulnerability scanning. 4. EXPECTED AWS COST BREAKDOWN The following Cost Estimation is based on the Asia Pacific (Singapore) region, which is the standard region for low latency access from Vietnam.\nCategory Service Estimated Configuration Monthly Cost (USD) Network NAT Gateway 1 NAT Gateway (Required for Private Subnet) + Data Processing ~$43.13 VPC Subnets, Security Groups ~$13.14 CloudFront 5GB Data Transfer (Utilizing Free Tier) ~$3.00 Compute Lambda 1,000 requests, 512MB RAM (Free Tier) ~$0.00 API Gateway 1,000 requests ~$0.00 Database RDS db.t3.micro, 20GB Storage ~$21.74 Storage S3 5GB Storage, 200 requests ~$0.14 Hosting Amplify Build \u0026amp; Hosting, WAF enabled ~$16.77 Ops CloudWatch Logs, Metrics, Alarms ~$9.41 CI/CD CodePipeline 1 Active Pipeline ~$1.05 Total ~$108.38 / Month 5. TEAM AWS FCJ Program Lead (Mentor)\nName Title Description Email / Contact Info Nguyễn Gia Hưng Head of Solutions Architect Provides technical mentorship, architecture review, and AWS best practices guidance. hunggia@amazon.com Project Stakeholders\nName Title Stakeholder for Email / Contact Info AWS FCJ Mentors Program Instructor Academic oversight, project evaluation, internship credit. Internship Team (FPT University)\nName Title Role Email / Contact Info Nguyễn Đức Lân Co-Lead (PM) Project Manager: Responsible for team coordination, progress tracking, code structure, UAT, and Cost Optimization. lannguyen68609@gmail.com Nguyễn Công Minh Co-Lead (Technical) DevOps \u0026amp; Backend: CI/CD, CodePipeline, CDK Stack, Lambda implementation. minhncse182968@fpt.edu.vn Đỗ Khang Member Cloud Architect: Architect Design, Service Policy, Co-designed AWS serverless architecture. dokhang307@gmail.com Lê Hoàng Anh Member Fullstack Dev: API implementation, UI/UX design, Security configurations. anhlhse170327@fpt.edu.vn Nguyễn Quách Lam Giang Member Data Engineering: RDS with MySQL connection, VPC \u0026amp; Subnet configuration. nguyenlamgiang2198@gmail.com Project Escalation Contacts\nName Title Role Email / Contact Info Nguyễn Đức Lân Team Lead Primary contact for project status and escalations. lannguyen68609@gmail.com 6. RESOURCES \u0026amp; COST ESTIMATES Resource Breakdown\nResource Responsibility Nguyễn Đức Lân Coordination \u0026amp; PM: Project Management, Cost Optimization, Forensic analysis, System configuration, and Full-stack development support. Nguyễn Công Minh DevOps \u0026amp; Infrastructure: CI/CD pipeline, Security, CDK Stack, Lambda implementation, Co-design API structure, System infrastructure. Lê Hoàng Anh Frontend \u0026amp; UI/UX: Frontend development, API Integration, UI/UX Design, Application Security. Nguyễn Quách Lam Giang Data Engineering: Data Analysis, RDS to MySQL connection, VPC creation, CloudWatch monitoring, Subnet and NAT Gateway configuration. Đỗ Khang Cloud Architecture: Architecture design, Service Policies, Documentation, API structure co-design, AI Chatbot integrations, CloudWatch monitoring. Hours by Project Phase\nProject Phase N.Đ. Lân N.C. Minh L.H. Anh N.Q.L. Giang Đ. Khang Total Hours Foundation 30 30 30 30 30 150 Core Orchestration 30 30 30 30 30 150 Analytics Layer 30 30 30 30 30 150 Testing \u0026amp; Validation 30 30 30 30 30 150 Documentation \u0026amp; Handover 30 30 30 30 30 150 Total Hours 150 150 150 150 150 750 Cost Distribution\nParty Contribution (USD) % Contribution of Total AWS FCJ Program $0 (Non-profit internship) 0% FPT University Student labor (Academic Credit) 0% AWS Infrastructure ~$15.00 (Testing \u0026amp; Running Costs) 100% Total Project Cost ~$15.00 100% *Note on Cost Efficiency: The labor cost for this project is subsidized as part of the FPT University Internship and AWS First Cloud Journey (FCJ) program. The estimated $15 represents the infrastructure running costs (AWS Credits) required for development and testing environments.\n7. ACCEPTANCE 7.1 Submission of Deliverables: Upon completion of the final phase (\u0026ldquo;Handover\u0026rdquo;), Skyline Team (PROVIDER) will submit the associated tangible Deliverables (Source Code, AWS Architecture Documentation, Admin Credentials, and Operational Manual) to the Project Stakeholders/Instructors (CUSTOMER). This submission will be accompanied by an Acceptance Form (or a formal Acceptance Email).\n7.2 Acceptance Period \u0026amp; Review: Upon such submission, the Customer will review, evaluate, and perform User Acceptance Testing (UAT) on the applicable Deliverable(s) within five (5) business days (the “Acceptance Period”). The review will determine whether each Deliverable satisfies the Project Success Criteria (defined in Section 1.2) and adheres to the AWS Architecture (defined in Section 2.1) in all material respects.\n7.3 Confirmation of Acceptance: If the Deliverable satisfies its acceptance criteria, the Customer will furnish a written acceptance confirmation to the Provider via the Acceptance Form prior to the end of the Acceptance Period. This signature (or written confirmation) marks the official closure of the project.\n7.4 Rejection \u0026amp; Remediation: For a Deliverable that is not accepted due to a non-conformity (e.g., Critical bugs, Security vulnerabilities, or missing features compared to the Scope of Work), the Customer will indicate the detailed reasons for such rejection on the Acceptance Form (a “Rejection Notice”) within the Acceptance Period.\nUpon receipt of a Rejection Notice, the Provider will promptly correct any defects or non-conformities to the extent required. Thereafter, the Provider will resubmit the modified Deliverable to the Customer, and the acceptance process set forth above will be repeated.\n7.5 Scope of Re-evaluation: The Customer will limit its review of resubmitted Deliverables to determining whether or not the Provider has corrected the defects identified in the Rejection Notice and to the effects which these corrections have on other portions of the system.\n7.6 Deemed Acceptance: If the Customer fails to provide the Provider with the above-described Rejection Notice or signed Acceptance Form prior to the end of the applicable Acceptance Period, then the corresponding Deliverable(s) and the project as a whole are deemed accepted.\n"},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.7-blooddonation-workshop/5.7.2-deploy-backend/","title":"Deploy Backend (Lambda + API Gateway)","tags":[],"description":"","content":"This section shows two ways to deploy the backend service from the repository: (A) manual packaging and updating an existing Lambda, and (B) using the provided buildspec.yml with CodeBuild/CodePipeline.\nA. Manual packaging and deploy (quick test)\nChange to the backend directory and install dependencies: cd aws-blood_donation_cloud/BloodDonationSupportSystemBE npm ci npm run build Create a deployment package (zip) suitable for Lambda: cd dist npm install --production --ignore-scripts zip -r ../deployment-package.zip . cd .. Create or update a Lambda function (example): # create (one-time) aws lambda create-function --function-name DaiVietBloodBackend \\ --runtime nodejs18.x --role arn:aws:iam::123456789012:role/your-lambda-role \\ --handler src/lambda.handler --zip-file fileb://deployment-package.zip # update code (subsequent deployments) aws lambda update-function-code --function-name DaiVietBloodBackend --zip-file fileb://deployment-package.zip Configure environment variables and API Gateway mapping as required by the project (DB connection string, Cognito settings, etc.). Validation: invoke the Lambda or call the API endpoint to ensure the function runs and returns expected responses.\nB. CI/CD using CodeBuild \u0026amp; CodePipeline (recommended for repeatability)\nReview buildspec.yml in the repo root — it contains the build steps used by CodeBuild to install, build and package the backend.\nCreate an S3 bucket for pipeline artifacts and a CodePipeline that uses GitHub as source (or GitHub Actions). Configure CodeBuild project to use the buildspec.yml file.\nEnsure the CodeBuild service role has permissions to update Lambda and upload artifacts to S3.\nNotes \u0026amp; Troubleshooting\nIf Lambda fails on cold start or dependencies, check that native modules are built for Amazon Linux (build in a compatible environment or use a CodeBuild image). For database access from Lambda inside a VPC, ensure subnets and security groups allow outbound access to the RDS instance. CloudFormation Quickstart (minimal) The following CloudFormation snippet provides a minimal starting point you can use as a template: it provisions a VPC with public/private subnets, a security group for Lambda \u0026lt;-\u0026gt; RDS access, an IAM role for Lambda with basic execution permissions, and a Lambda function resource which expects an S3-stored deployment package. Replace placeholder values (BucketName, Key, RoleArn) before use.\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: Minimal resources for DaiVietBlood workshop (VPC, SG, Lambda role, Lambda) Resources: VPC: Type: AWS::EC2::VPC Properties: CidrBlock: 10.0.0.0/16 Tags: - Key: Name Value: DaiVietBlood-VPC PublicSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !Ref VPC CidrBlock: 10.0.1.0/24 AvailabilityZone: !Select [0, !GetAZs \u0026#34;\u0026#34;] PrivateSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !Ref VPC CidrBlock: 10.0.2.0/24 AvailabilityZone: !Select [0, !GetAZs \u0026#34;\u0026#34;] LambdaSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Allow outbound to RDS VpcId: !Ref VPC LambdaExecutionRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Effect: Allow Principal: Service: lambda.amazonaws.com Action: sts:AssumeRole Path: / Policies: - PolicyName: LambdaBasicExecution PolicyDocument: Version: \u0026#39;2012-10-17\u0026#39; Statement: - Effect: Allow Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents Resource: arn:aws:logs:*:*:* DaiVietBloodLambda: Type: AWS::Lambda::Function Properties: FunctionName: DaiVietBloodBackend Handler: src/lambda.handler Runtime: nodejs18.x Role: !GetAtt LambdaExecutionRole.Arn Code: S3Bucket: YOUR_DEPLOYMENT_BUCKET_NAME S3Key: deployment-package.zip VpcConfig: SecurityGroupIds: - !Ref LambdaSecurityGroup SubnetIds: - !Ref PrivateSubnet1 Outputs: LambdaArn: Value: !GetAtt DaiVietBloodLambda.Arn Description: ARN of the backend Lambda function Use this template as a starting point; production deployments should use multiple AZ subnets, NAT gateways for outbound access, and finer-grained IAM policies.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Solution Architecture: Refine both High-Level (HLD) and Low-Level (LLD) diagrams aligned with the AWS Well-Architected Framework. Resource Planning \u0026amp; Cost Estimation: Develop an understanding of Total Cost of Ownership (TCO) and evaluate service options for efficiency. Network Foundation: Lay down the initial networking environment including VPCs, subnets, and basic security mechanisms. Activities Undertaken This Week: Day Activity Start Date Completion Date Reference 2-3 Architecture \u0026amp; Planning:\n- Reviewed business requirements and identified critical components.\n- Drafted system architecture diagrams illustrating resource flows.\n- Estimated costs using AWS Pricing Calculator and considered cost-saving measures.\n- Checked alignment with the 5 pillars of the Well-Architected Framework. 10/11/2025 11/11/2025 4-5 Infrastructure Initialization (IaC):\n- Developed Terraform/CloudFormation scripts to provision VPC, Public/Private Subnets, NAT Gateway, and Route Tables.\n- Configured Security Groups for Bastion Hosts, Application Servers, and Databases following best practices. 12/11/2025 13/11/2025 Week 10 Reflection: Architecture and Network Foundations 1. Architecture Design Insights Diagrammatic Representation: Successfully completed data flow and resource layout diagrams, following a Single-AZ setup for simplicity during the initial phase. Service Decisions: Relational Database: Chose RDS MySQL with Multi-AZ for resilience. Storage: S3 for static files and backups. Cost Awareness: Developed a preliminary monthly cost estimate and identified optimization strategies, such as leveraging Spot Instances in development and applying Savings Plans in production environments. 2. Networking \u0026amp; Security Foundation VPC Setup: Established a well-structured VPC with clear separation between Public and Private subnets for applications and databases. Security Implementation: Applied least privilege principles in Security Group configurations to protect bastion, web, and database layers. Overall, this week laid the groundwork for both architectural clarity and a secure, operational network environment, setting the stage for further deployments and service integrations.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Core Service Deployment: Configure foundational compute and database resources to support the application. Automation \u0026amp; DevOps: Establish a CI/CD pipeline to streamline build and deployment processes for cloud-based environments. Activities Undertaken This Week: Day Activity Start Date Completion Date Reference 2-3 Compute \u0026amp; Database Setup:\n- Provisioned RDS instance (initially Single-AZ, later expanded to Multi-AZ for resilience).\n- Established connectivity between application servers and MySQL database. 17/11/2025 18/11/2025 4-5 CI/CD Pipeline Implementation:\n- Configured source repository (CodeCommit/GitHub).\n- Automated build processes via CodeBuild.\n- Integrated CodeDeploy and CodePipeline for automated deployments to EC2 instances.\n- Verified end-to-end deployment flow from local code to cloud environment. 19/11/2025 20/11/2025 Week 11 Reflection: Operational Environment \u0026amp; Automation 1. Application Availability \u0026amp; Database Connectivity RDS Stability: Database now operates in Multi-AZ mode, improving fault tolerance and data durability. Application Integration: Application servers successfully connected to the database, enabling data-driven functionalities. 2. DevOps \u0026amp; Continuous Deployment Pipeline Realization: Achieved a fully automated CI/CD workflow: Source Control: Commits pushed to GitHub or CodeCommit automatically trigger builds. Build Process: Dependencies are installed, artifacts packaged, and tested in an automated fashion. Deployment: New versions are rolled out to EC2 instances with minimal disruption, implementing zero-downtime deployment strategies such as rolling updates. This week established a robust operational foundation, combining resilient database setup with automated deployment practices, setting the stage for scalable and maintainable cloud application operations.\n"},{"uri":"https://docvoxx.github.io/internship-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Security \u0026amp; Monitoring: Scan for security vulnerabilities and set up a system monitoring Dashboard. Project Wrap-up: Finalize documentation, perform final optimizations, and prepare presentation slides. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2-3 - Configure CloudWatch Alarms and Dashboard.\n- Perform security review using AWS Trusted Advisor and GuardDuty. 24/11/2025 25/11/2025 4-5 Optimization \u0026amp; Packaging:\n- Review and delete redundant resources for cost optimization.\n- Write operational documentation (Runbook).\n- Draft final report slides (Architecture, Challenges, Lessons Learned). 26/11/2025 27/11/2025 Week 12 Achievements: System Finalization \u0026amp; Handover Readiness 1. Performance Evaluation \u0026amp; Tuning Monitoring: Established a CloudWatch Dashboard to visually display key metrics: CPU Utilization, Request Count, Database Connections, Error Rate (4xx, 5xx). Configured SNS Alerts to send email notifications to Admins when system issues occur. 2. Optimization \u0026amp; Advanced Security Security: Enabled WAF (Web Application Firewall) to block common attacks (SQL Injection, XSS). Reviewed IAM Roles and revoked unnecessary privileges. Cost: Based on data from the 2-week pilot run, resized Instances from t3.medium to t3.micro for the Dev environment to save costs without impacting performance. 3. Project Completion Documentation: Completed the project documentation suite including: Updated Architecture Diagram, Redeployment Guide, and Cost Report. Lessons Learned: Gained insights into handling \u0026ldquo;Cold Start\u0026rdquo; issues during Auto Scaling and the importance of accurate Health Check configuration. "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test API Endpoints","tags":[],"description":"","content":"Step 1: Test from API Gateway Console 1.1. Test GET /users\nGo to API Gateway Console → Select daivietblood-api Select /users → GET Click Test Click Test button Expected response:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;[]\u0026#34; } Step 2: Test with cURL Replace YOUR_API_URL with your actual Invoke URL.\n2.1. Create a User (POST /users)\ncurl -X POST https://YOUR_API_URL/prod/users \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; } 2.2. Get All Users (GET /users)\ncurl https://YOUR_API_URL/prod/users Expected response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-12-09T10:00:00.000Z\u0026#34; } ] 2.3. Create Emergency Request (POST /emergency-requests)\ncurl -X POST https://YOUR_API_URL/prod/emergency-requests \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;requester_name\u0026#34;: \u0026#34;Benh vien Cho Ray\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;AB-\u0026#34;, \u0026#34;units_needed\u0026#34;: 5, \u0026#34;hospital\u0026#34;: \u0026#34;Cho Ray Hospital\u0026#34;, \u0026#34;urgency\u0026#34;: \u0026#34;critical\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;message\u0026#34;: \u0026#34;Emergency request created\u0026#34; } 2.4. Get Emergency Requests (GET /emergency-requests)\ncurl https://YOUR_API_URL/prod/emergency-requests Step 3: Test with Postman Open Postman Create new Collection: DaiVietBlood API Add requests: Request Name Method URL Get Users GET {{baseUrl}}/users Create User POST {{baseUrl}}/users Get Emergency Requests GET {{baseUrl}}/emergency-requests Create Emergency Request POST {{baseUrl}}/emergency-requests Set Collection variable: baseUrl: https://YOUR_API_URL/prod Step 4: Verify Lambda Logs Go to CloudWatch Console → Log groups\nFind log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Check recent log streams for:\nSuccessful invocations Any errors or exceptions Database connection logs Common Issues \u0026amp; Solutions Issue Cause Solution 502 Bad Gateway Lambda error Check CloudWatch logs for details Timeout Lambda cannot reach RDS Verify VPC, Subnets, Security Groups CORS error CORS not configured Enable CORS on API Gateway 500 Internal Server Error Database connection failed Check DB credentials in environment variables Step 5: Performance Check Note the response time for each API call First call may be slow (Lambda cold start) Subsequent calls should be faster Expected performance:\nEndpoint Cold Start Warm GET /users ~3-5s ~200-500ms POST /users ~3-5s ~200-500ms GET /emergency-requests ~3-5s ~200-500ms 💡 Tip: Lambda cold start in VPC can be slow. Consider using Provisioned Concurrency for production workloads.\nVerification Checklist GET /users returns empty array or user list POST /users creates new user successfully GET /emergency-requests returns requests list POST /emergency-requests creates new request No CORS errors in browser console CloudWatch logs show successful invocations "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.3-s3-vpc/","title":"VPC &amp; Amazon RDS","tags":[],"description":"","content":"In this section, you will create the network infrastructure (VPC) and database (RDS) for the DaiVietBlood system.\nArchitecture Overview Content Create VPC Create Amazon RDS "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.7-blooddonation-workshop/5.7.3-deploy-frontend/","title":"Deploy Frontend (Amplify)","tags":[],"description":"","content":"The repository contains a frontend project that can be hosted with AWS Amplify or any static hosting (S3 + CloudFront). This section covers deploying with Amplify.\nFrom the AWS Console, open AWS Amplify and choose \u0026ldquo;Get started\u0026rdquo; → \u0026ldquo;Host web app\u0026rdquo;. Connect the GitHub repository (june4m/aws-blood_donation_cloud) and choose the frontend directory (if present). Configure a build setting if Amplify cannot auto-detect the framework. Alternatively, build locally and deploy to S3: # build (example) cd path/to/frontend npm ci npm run build # sync build folder to S3 aws s3 sync build/ s3://your-frontend-bucket --delete Configure CloudFront and custom domain as needed. Validation: Open the Amplify URL or CloudFront distribution to verify the frontend loads and interacts with the backend API.\n"},{"uri":"https://docvoxx.github.io/internship-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Configure CORS &amp; Security","tags":[],"description":"","content":"Understanding CORS CORS (Cross-Origin Resource Sharing) is a security feature that restricts web pages from making requests to a different domain than the one serving the web page.\nWhen your React frontend (hosted on Amplify) calls your API Gateway, the browser checks CORS headers to determine if the request is allowed.\nStep 1: Configure CORS Headers in Lambda Ensure all Lambda functions return proper CORS headers:\nconst corsHeaders = { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, // Or specific domain \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;GET,POST,PUT,DELETE,OPTIONS\u0026#39; }; // In your handler response: return { statusCode: 200, headers: corsHeaders, body: JSON.stringify(data) }; Step 2: Configure API Gateway CORS Method 1: Using Console\nGo to API Gateway Console → Select your API For each resource: Select resource → Actions → Enable CORS Configure allowed origins, methods, headers Click Enable CORS and replace existing CORS headers Method 2: Using OPTIONS Method\nCreate OPTIONS method for each resource Integration type: Mock Add Method Response with status 200 Add Integration Response with headers: Access-Control-Allow-Headers: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#39; Access-Control-Allow-Methods: \u0026#39;GET,POST,OPTIONS\u0026#39; Access-Control-Allow-Origin: \u0026#39;*\u0026#39; Step 3: API Gateway Security Best Practices 3.1. Enable API Key (Optional)\nGo to API Gateway → API Keys → Create API Key Name: daivietblood-api-key Go to Usage Plans → Create Configure throttling and quota Associate API Key with Usage Plan For each method, set API Key Required: true 3.2. Enable Request Validation\nGo to API Gateway → Models → Create Create model for request body: { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-04/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CreateUserModel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;blood_type\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;blood_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;A+\u0026#34;, \u0026#34;A-\u0026#34;, \u0026#34;B+\u0026#34;, \u0026#34;B-\u0026#34;, \u0026#34;AB+\u0026#34;, \u0026#34;AB-\u0026#34;, \u0026#34;O+\u0026#34;, \u0026#34;O-\u0026#34;] }, \u0026#34;phone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } Apply model to POST method: Select method → Method Request Request Validator: Validate body Request Body: Add model 3.3. Enable Throttling\nGo to Stages → Select prod Stage Settings → Default Method Throttling Configure: Rate: 100 requests/second Burst: 200 requests Step 4: Lambda Security Best Practices 4.1. Use AWS Secrets Manager for Credentials\nInstead of storing DB credentials in environment variables:\nGo to Secrets Manager → Store a new secret\nSecret type: Other type of secret\nKey/value pairs:\nDB_HOST: daivietblood-db.xxxx.rds.amazonaws.com DB_USER: admin DB_PASSWORD: YourSecurePassword123! DB_NAME: daivietblood Secret name: daivietblood/db-credentials\nUpdate Lambda to retrieve secrets:\nconst { SecretsManagerClient, GetSecretValueCommand } = require(\u0026#39;@aws-sdk/client-secrets-manager\u0026#39;); const client = new SecretsManagerClient({ region: \u0026#39;ap-southeast-1\u0026#39; }); const getDbCredentials = async () =\u0026gt; { const command = new GetSecretValueCommand({ SecretId: \u0026#39;daivietblood/db-credentials\u0026#39; }); const response = await client.send(command); return JSON.parse(response.SecretString); }; Add IAM permission to Lambda role: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:ap-southeast-1:*:secret:daivietblood/*\u0026#34; } 4.2. Input Validation\nAlways validate input in Lambda:\nconst validateUser = (body) =\u0026gt; { const errors = []; if (!body.email || !isValidEmail(body.email)) { errors.push(\u0026#39;Invalid email\u0026#39;); } if (!body.name || body.name.length \u0026lt; 1) { errors.push(\u0026#39;Name is required\u0026#39;); } const validBloodTypes = [\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;]; if (!validBloodTypes.includes(body.blood_type)) { errors.push(\u0026#39;Invalid blood type\u0026#39;); } return errors; }; Step 5: Redeploy API After making changes:\nActions → Deploy API Select prod stage Click Deploy Security Checklist CORS configured correctly Lambda returns proper CORS headers API Key enabled (optional but recommended) Request validation enabled Throttling configured DB credentials stored in Secrets Manager (recommended) Input validation in Lambda functions API redeployed after changes "},{"uri":"https://docvoxx.github.io/internship-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day Vietnam\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Data Resiliency in a Cloud - First World\nDate \u0026amp; Time: 09:00, October 14, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Data Science on AWS\nDate \u0026amp; Time: 09:00, October 16, 2025\nLocation: Hall B, FPT University Ho Chi Minh, District 9, Thu Duc, Ho Chi Minh\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.4-s3-onprem/","title":"Lambda &amp; API Gateway","tags":[],"description":"","content":"In this section, you will create AWS Lambda functions and expose them via Amazon API Gateway to build the serverless backend for DaiVietBlood.\nArchitecture Overview API Endpoints Method Endpoint Description GET /users Get all users POST /users Create new user GET /users/{id} Get user by ID GET /donations Get all donations POST /donations Create donation appointment GET /emergency-requests Get emergency requests POST /emergency-requests Create emergency request Content Create Lambda Functions Create API Gateway Test API Endpoints Configure CORS \u0026amp; Security "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.7-blooddonation-workshop/5.7.4-database/","title":"Database Setup (RDS &amp; Seeds)","tags":[],"description":"","content":"This section covers provisioning an Amazon RDS instance for the project and loading the provided SQL schema and seed data.\nCreate an RDS instance (MySQL or PostgreSQL as the project requires). Choose db.t3.micro (Free Tier eligible) for testing.\nConfigure networking: place RDS in private subnets within a VPC. Ensure security group allows inbound traffic from the application (Lambda security group or bastion host IP for manual access).\nInitialize the database with schema and seed data:\n# From repo root aws s3 cp BloodDonationSupportSystem.sql s3://your-bucket/ || true # (or run locally against RDS using mysql client) mysql -h \u0026lt;rds-endpoint\u0026gt; -u \u0026lt;user\u0026gt; -p \u0026lt; BloodDonationSupportSystem.sql mysql -h \u0026lt;rds-endpoint\u0026gt; -u \u0026lt;user\u0026gt; -p \u0026lt; initData.sql For Lambda connectivity: ensure DB credentials are stored securely (AWS Secrets Manager or Parameter Store). Provide the Lambda function with permissions to read those secrets. Validation: Connect with a MySQL/Postgres client and run a few SELECTs to confirm schema and seeded data present.\nNotes\nKeep RDS in private subnets and use NAT for outbound connectivity if needed. Consider using RDS Proxy for serverless backends to manage connections. "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.5-policy/","title":"S3, CloudFront &amp; Amplify","tags":[],"description":"","content":"In this section, you will set up Amazon S3 for static assets, CloudFront for content distribution, and AWS Amplify to host the React frontend application.\nArchitecture Overview Part 1: Amazon S3 Setup Step 1: Create S3 Bucket for Assets Go to S3 Console → Create bucket\nGeneral configuration:\nBucket name: daivietblood-assets-{your-account-id} AWS Region: Asia Pacific (Singapore) ap-southeast-1 Object Ownership:\nACLs disabled (recommended) Block Public Access settings:\nBlock all public access: ✅ (We\u0026rsquo;ll use CloudFront) Bucket Versioning:\nEnable (recommended for production) Default encryption:\nServer-side encryption: Enable Encryption type: Amazon S3 managed keys (SSE-S3) Click Create bucket\nStep 2: Create Bucket Policy for CloudFront After creating CloudFront distribution (Part 2), update bucket policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::daivietblood-assets-{your-account-id}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::{account-id}:distribution/{distribution-id}\u0026#34; } } } ] } Step 3: Upload Sample Assets Create folder structure:\n/images /blood-types /icons /banners /documents Upload sample images for the application\nPart 2: CloudFront Setup Step 1: Create CloudFront Distribution Go to CloudFront Console → Create distribution\nOrigin settings:\nOrigin domain: Select your S3 bucket Origin path: Leave empty Name: daivietblood-s3-origin Origin access: Origin access control settings (recommended) Create new OAC: Click Create control setting Name: daivietblood-oac Signing behavior: Sign requests Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD Cache policy: CachingOptimized Settings:\nPrice class: Use only North America and Europe (or All edge locations) Default root object: index.html Click Create distribution\nImportant: Copy the bucket policy provided and update your S3 bucket policy\nStep 2: Get CloudFront Domain After distribution is deployed (takes 5-10 minutes):\nCopy the Distribution domain name:\nhttps://d1234567890.cloudfront.net Test accessing an asset:\nhttps://d1234567890.cloudfront.net/images/logo.png Part 3: AWS Amplify Setup Step 1: Prepare React Application Create React app (if not exists): npx create-react-app daivietblood-frontend cd daivietblood-frontend Install dependencies: npm install axios react-router-dom Create .env file: REACT_APP_API_URL=https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod REACT_APP_ASSETS_URL=https://d1234567890.cloudfront.net Sample API service (src/services/api.js): import axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL; export const getUsers = async () =\u0026gt; { const response = await axios.get(`${API_URL}/users`); return response.data; }; export const createUser = async (userData) =\u0026gt; { const response = await axios.post(`${API_URL}/users`, userData); return response.data; }; export const getEmergencyRequests = async () =\u0026gt; { const response = await axios.get(`${API_URL}/emergency-requests`); return response.data; }; export const createEmergencyRequest = async (requestData) =\u0026gt; { const response = await axios.post(`${API_URL}/emergency-requests`, requestData); return response.data; }; Push to GitHub repository Step 2: Deploy with Amplify Go to AWS Amplify Console → Create new app\nChoose source:\nGitHub → Continue Authorize AWS Amplify to access your GitHub Add repository branch:\nRepository: Select your repository Branch: main Configure build settings:\nApp name: daivietblood-frontend Build and test settings: Auto-detected for React Build settings (amplify.yml):\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: build files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* Environment variables:\nAdd REACT_APP_API_URL and REACT_APP_ASSETS_URL Click Save and deploy\nStep 3: Configure Custom Domain (Optional) Go to App settings → Domain management Click Add domain Enter your domain name Configure DNS records as instructed Part 4: Verify Deployment Access Amplify URL:\nhttps://main.d1234567890.amplifyapp.com Test functionality:\nHomepage loads correctly API calls work (check Network tab) Images load from CloudFront No CORS errors Verification Checklist S3 bucket created with proper settings CloudFront distribution deployed S3 bucket policy updated for CloudFront access Assets accessible via CloudFront URL React app deployed to Amplify Environment variables configured Frontend can call API Gateway Images load from CloudFront "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nBuilding Serverless System on AWS - DaiVietBlood Overview This workshop guides you through building a Serverless Blood Donation \u0026amp; Emergency System (DaiVietBlood) on AWS. You will learn how to set up and configure the core AWS services used in the project architecture.\nAWS Services Used Service Purpose Amazon VPC Create virtual private network with Public/Private Subnets NAT Gateway Allow resources in Private Subnet to access Internet Amazon RDS MySQL database for the application AWS Lambda Serverless business logic processing Amazon API Gateway Manage and expose REST APIs Amazon S3 Store static assets (images, files) Amazon CloudFront CDN for global content distribution AWS Amplify Host Frontend application (React) AWS CodePipeline CI/CD automation Amazon CloudWatch Monitoring and logging What You Will Learn Design and deploy Serverless-First architecture on AWS Configure VPC with Public/Private Subnets for security Create RDS MySQL in Private Subnet Build Lambda Functions and connect with API Gateway Store and distribute content with S3 and CloudFront Deploy React application with AWS Amplify Set up automated CI/CD Pipeline Monitor application with CloudWatch Prerequisites AWS Account with Administrator access Basic knowledge of AWS services Familiarity with Node.js and React AWS CLI installed and configured Estimated Cost This workshop uses resources within AWS Free Tier when possible. Estimated cost is approximately ~$15-20 if completed within 1-2 days and resources are cleaned up immediately after.\nContent Workshop Overview Preparation VPC \u0026amp; Amazon RDS Lambda \u0026amp; API Gateway S3, CloudFront \u0026amp; Amplify CI/CD, CloudWatch \u0026amp; Cleanup "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.7-blooddonation-workshop/5.7.5-cicd/","title":"CI/CD (CodePipeline &amp; CodeBuild)","tags":[],"description":"","content":"This section outlines how to set up CI/CD for the repo using AWS CodePipeline and CodeBuild. The repo includes a buildspec.yml that demonstrates install/build/package steps for the backend.\nCreate an S3 bucket for pipeline artifacts. Create or connect a CodePipeline that uses GitHub (or CodeCommit) as the source. Create a CodeBuild project and point it to the repo root (it will pick buildspec.yml). Use an image with Node.js 18. Grant the CodeBuild role permissions to update Lambda (lambda:UpdateFunctionCode), upload artifacts to S3, and to other services used by the build steps. Tip: Reuse the buildspec.yml found in the repository. Test the build locally by running the same commands in the phases section.\nValidation: Push a test commit and verify the pipeline runs, builds artifacts and updates Lambda or uploads the deployment package to S3.\nIAM Policy Examples Below are example IAM policy snippets to help you scope permissions for CodeBuild (CI role) and for Lambda (execution role). These are conservative examples — adjust resource ARNs and limits to fit your account.\nCodeBuild service role example (minimal permissions to build, upload artifacts, and update Lambda):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::YOUR_ARTIFACT_BUCKET\u0026#34;, \u0026#34;arn:aws:s3:::YOUR_ARTIFACT_BUCKET/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:UpdateFunctionConfiguration\u0026#34;, \u0026#34;lambda:PublishVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:DaiVietBloodBackend\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } Lambda execution role example (allowing access to Secrets Manager and RDS connectivity):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:REGION:ACCOUNT_ID:secret:YOUR_SECRET_NAME*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rds-db:connect\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:rds-db:REGION:ACCOUNT_ID:dbuser:DBRESOURCE/YOUR_DB_USER\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } Replace REGION, ACCOUNT_ID, YOUR_ARTIFACT_BUCKET, and other placeholders with your actual values. Prefer least privilege (narrow resource ARNs) in production.\n"},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.6-cleanup/","title":"CI/CD, CloudWatch &amp; Cleanup","tags":[],"description":"","content":"In this final section, you will set up CI/CD Pipeline, configure CloudWatch monitoring, and clean up all resources after completing the workshop.\nPart 1: CI/CD Pipeline with CodePipeline Step 1: Create CodeBuild Project Go to CodeBuild Console → Create build project\nProject configuration:\nProject name: daivietblood-backend-build Description: Build project for Lambda functions Source:\nSource provider: GitHub Repository: Select your repository Branch: main Environment:\nEnvironment image: Managed image Operating system: Amazon Linux 2 Runtime: Standard Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Service role: New service role Buildspec:\nBuild specifications: Use a buildspec file Create buildspec.yml file in your repository: version: 0.2 phases: install: runtime-versions: nodejs: 18 commands: - echo Installing dependencies... - cd backend \u0026amp;\u0026amp; npm ci pre_build: commands: - echo Running tests... - npm test || true build: commands: - echo Building Lambda packages... - mkdir -p dist - zip -r dist/get-users.zip functions/get-users/ - zip -r dist/create-user.zip functions/create-user/ - zip -r dist/emergency-requests.zip functions/emergency-requests/ post_build: commands: - echo Updating Lambda functions... - aws lambda update-function-code --function-name daivietblood-get-users --zip-file fileb://dist/get-users.zip - aws lambda update-function-code --function-name daivietblood-create-user --zip-file fileb://dist/create-user.zip - aws lambda update-function-code --function-name daivietblood-emergency-requests --zip-file fileb://dist/emergency-requests.zip artifacts: files: - dist/**/* Click Create build project Step 2: Create CodePipeline Go to CodePipeline Console → Create pipeline\nPipeline settings:\nPipeline name: daivietblood-pipeline Service role: New service role Source stage:\nSource provider: GitHub (Version 2) Connection: Create new connection or select existing Repository name: Select your repository Branch name: main Output artifact format: CodePipeline default Build stage:\nBuild provider: AWS CodeBuild Project name: daivietblood-backend-build Deploy stage:\nSkip deploy stage (Lambda is updated in build stage) Click Create pipeline\nStep 3: Add IAM Permissions for CodeBuild Go to IAM Console → Roles Find role codebuild-daivietblood-backend-build-service-role Add inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:*:function:daivietblood-*\u0026#34; } ] } Part 2: CloudWatch Monitoring Step 1: Create CloudWatch Dashboard Go to CloudWatch Console → Dashboards → Create dashboard\nDashboard name: DaiVietBlood-Monitoring\nAdd widgets:\nWidget 1: Lambda Invocations\nWidget type: Line Metrics: Lambda → By Function Name → Invocations Select all daivietblood functions Widget 2: Lambda Errors\nWidget type: Number Metrics: Lambda → By Function Name → Errors Statistic: Sum Widget 3: Lambda Duration\nWidget type: Line Metrics: Lambda → By Function Name → Duration Statistic: Average Widget 4: API Gateway Requests\nWidget type: Line Metrics: ApiGateway → By Api Name → Count Widget 5: RDS Connections\nWidget type: Line Metrics: RDS → Per-Database Metrics → DatabaseConnections Step 2: Create CloudWatch Alarms Alarm 1: Lambda Errors\nGo to CloudWatch → Alarms → Create alarm Select metric: Lambda → By Function Name → Errors Conditions: Threshold type: Static Whenever Errors is: Greater than 5 Period: 5 minutes Notification: Create new SNS topic: daivietblood-alerts Email: your-email@example.com Alarm name: DaiVietBlood-Lambda-Errors Alarm 2: RDS CPU High\nCreate alarm Select metric: RDS → Per-Database Metrics → CPUUtilization Conditions: Threshold: Greater than 80% Period: 5 minutes Notification: Use existing SNS topic Alarm name: DaiVietBlood-RDS-CPU-High Alarm 3: API Gateway 5XX Errors\nCreate alarm Select metric: ApiGateway → By Api Name → 5XXError Conditions: Threshold: Greater than 10 Period: 5 minutes Alarm name: DaiVietBlood-API-5XX-Errors Step 3: Configure Log Insights Go to CloudWatch → Logs → Logs Insights\nSelect log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Sample query - Find errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 50 Sample query - Duration statistics: fields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) by bin(1h) Part 3: Resource Cleanup ⚠️ Important: Follow these steps to avoid unexpected charges.\nCleanup Order (Important!) Clean up in the following order to avoid dependency errors:\nStep 1: Delete Amplify App Go to Amplify Console Select daivietblood-frontend Actions → Delete app Confirm deletion Step 2: Delete CloudFront Distribution Go to CloudFront Console Select distribution → Disable Wait for status to change to \u0026ldquo;Deployed\u0026rdquo; Select distribution → Delete Step 3: Delete S3 Buckets Go to S3 Console Select bucket daivietblood-assets-* Empty bucket first Then Delete bucket Step 4: Delete API Gateway Go to API Gateway Console Select daivietblood-api Actions → Delete Step 5: Delete Lambda Functions Go to Lambda Console Delete each function: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests Delete Lambda Layer: mysql2-layer Step 6: Delete RDS Instance Go to RDS Console → Databases Select daivietblood-db Actions → Delete Uncheck \u0026ldquo;Create final snapshot\u0026rdquo; Check \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; Type delete me to confirm Step 7: Delete VPC Resources Go to VPC Console\nDelete NAT Gateway:\nNAT Gateways → Select NAT Gateway → Delete Wait for status \u0026ldquo;Deleted\u0026rdquo; Release Elastic IP:\nElastic IPs → Select EIP → Release Delete VPC Endpoints (if any):\nEndpoints → Select endpoints → Delete Delete Security Groups (except default):\nSecurity Groups → Delete daivietblood-lambda-sg, daivietblood-rds-sg Delete DB Subnet Group:\nRDS Console → Subnet groups → Delete daivietblood-db-subnet-group Delete VPC:\nYour VPCs → Select daivietblood-vpc → Delete VPC This will delete subnets, route tables, internet gateway Step 8: Delete CI/CD Resources CodePipeline Console → Delete daivietblood-pipeline CodeBuild Console → Delete daivietblood-backend-build Step 9: Delete CloudWatch Resources CloudWatch → Dashboards → Delete DaiVietBlood-Monitoring CloudWatch → Alarms → Delete all related alarms CloudWatch → Log groups → Delete log groups /aws/lambda/daivietblood-* Step 10: Delete IAM Resources IAM Console → Roles Delete roles: daivietblood-lambda-role codebuild-daivietblood-* codepipeline-daivietblood-* Cleanup Checklist Amplify app deleted CloudFront distribution deleted S3 buckets emptied and deleted API Gateway deleted Lambda functions and layers deleted RDS instance deleted NAT Gateway deleted Elastic IP released VPC and all components deleted CodePipeline and CodeBuild deleted CloudWatch dashboards, alarms, log groups deleted IAM roles deleted Verify No Remaining Charges Go to AWS Cost Explorer Verify no resources are running Go to Billing Console → Bills to confirm 💡 Tip: Set up Budget Alert in AWS Budgets to receive notifications when costs exceed threshold.\nWorkshop Conclusion Congratulations! 🎉 You have completed the workshop on building a Serverless system on AWS.\nWhat You Learned: ✅ Design and deploy VPC with Public/Private Subnets ✅ Create RDS MySQL in a secure environment ✅ Build Lambda functions and expose via API Gateway ✅ Configure S3 and CloudFront for static assets ✅ Deploy React app with AWS Amplify ✅ Set up automated CI/CD Pipeline ✅ Monitor application with CloudWatch Next Steps: Learn more about AWS Well-Architected Framework Explore advanced features like X-Ray tracing Experiment with Aurora Serverless for database Implement authentication with Amazon Cognito "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.7-blooddonation-workshop/5.7.6-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Follow these steps to remove resources created during the workshop and avoid unexpected charges.\nDelete Lambda functions created for the workshop: aws lambda delete-function --function-name DaiVietBloodBackend Delete CodePipeline, CodeBuild projects and S3 artifact bucket. Delete RDS instance (take a final snapshot if you need to keep data): aws rds delete-db-instance --db-instance-identifier your-db --skip-final-snapshot Remove Amplify app or S3 bucket hosting the frontend. Revoke IAM roles created specifically for the workshop if no longer needed. Always double-check resource names before deletion to avoid removing shared resources.\n"},{"uri":"https://docvoxx.github.io/internship-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Company Limited from September 8, 2025, to December 28, 2025, I had the valuable opportunity to bridge the gap between academic knowledge and real-world application. This experience allowed me to develop both my technical skills and professional competencies in a corporate environment.\nI participated in 8 office sessions and 6 events during this period. Although the total number of activities was relatively limited, each session provided meaningful opportunities to network with peers and learn from their experiences. Office visits also enabled me to interact closely with mentors, particularly Mr. Thinh and Mr. Hoang Anh, with whom I maintained frequent communication and exchange of ideas.\nThroughout this internship, I have observed a gradual shift in my mindset—from a student perspective to a more mature, professional approach—adapting to the expectations and culture of a corporate environment. I consider this personal transformation essential for my future career development.\nIn terms of work ethic, I consistently aimed to complete tasks effectively, comply with workplace regulations, and actively collaborate with colleagues to enhance efficiency and productivity.\nTo objectively reflect on my performance, I evaluated myself across several key criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Areas for Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization. Enhance problem-solving skills and adopt a more analytical approach. Improve communication skills in both daily interactions and professional contexts, including handling situations effectively. "},{"uri":"https://docvoxx.github.io/internship-report/5-workshop/5.7-blooddonation-workshop/","title":"DaiVietBlood - Workshop","tags":[],"description":"","content":"Overview This hands-on workshop guides you through deploying the DaiVietBlood (Blood Donation Support System) reference project available at https://github.com/june4m/aws-blood_donation_cloud.\nThe workshop is organized so you can follow a minimum-privilege, cost-aware path from cloning the repository to running a simple end-to-end demo. It covers account and environment setup, backend packaging and deployment to AWS Lambda, frontend hosting, provisioning an RDS database and loading seed data, and configuring CI/CD using CodeBuild/CodePipeline.\nArchitecture Highlights Backend: Node.js Lambda functions packaged and deployed using the repository\u0026rsquo;s buildspec.yml (CodeBuild-friendly). Database: Relational schema provided (BloodDonationSupportSystem.sql, initData.sql) — intended for Amazon RDS (MySQL/Postgres). Frontend: Static app (can be hosted with AWS Amplify or S3 + CloudFront). CI/CD: Example buildspec.yml is included; CodePipeline/CodeBuild are recommended for reproducible builds and deployments. Workshop Content Workshop overview Prerequisite Deploy backend (Lambda + API Gateway) Deploy frontend (Amplify / S3+CloudFront) Database setup (RDS \u0026amp; seeds) CI/CD (CodePipeline / CodeBuild) Cleanup Each section contains step-by-step instructions, example commands, and validation checks. If you want, I can expand any section with CloudFormation templates, IAM policy examples, or exact parameter values for your environment.\n"},{"uri":"https://docvoxx.github.io/internship-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment was welcoming and supportive. Team members were always ready to assist whenever challenges arose, even outside regular hours. The workspace was well-organized and comfortable, which helped maintain focus. Adding more team-building activities or social events could further enhance engagement and collaboration.\n2. Support from Mentor / Team Admin\nThe mentor offered clear guidance and constructive feedback, encouraging independent problem-solving rather than simply providing answers. The admin team effectively handled logistics, provided essential documentation, and created conditions conducive to productivity. Their support made navigating the internship smoother and more manageable.\n3. Relevance of Work to Academic Major\nAssigned tasks closely aligned with my academic background while also introducing new domains I had not explored before. This combination allowed me to reinforce foundational knowledge and develop practical skills applicable in a professional environment.\n4. Learning \u0026amp; Skill Development Opportunities\nThe internship offered opportunities to learn new tools, improve teamwork abilities, and practice professional communication. Insights shared by the mentor about real-world applications helped me better understand industry practices and informed my career planning.\n5. Company Culture \u0026amp; Team Spirit\nThe organizational culture promoted mutual respect, diligence, and collaboration, while maintaining an enjoyable atmosphere. During high-priority projects, team members supported one another regardless of hierarchy, fostering a sense of inclusion and shared responsibility. This experience strengthened my sense of belonging and engagement within the team.\n"},{"uri":"https://docvoxx.github.io/internship-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://docvoxx.github.io/internship-report/tags/","title":"Tags","tags":[],"description":"","content":""}]